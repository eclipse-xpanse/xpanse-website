{"searchDocs":[{"title":"Open Services Cloud and GAIA-X, digital sovereignty","type":0,"sectionRef":"#","url":"/xpanse/blog/2022/07/20/gaiax","content":"As GAIA-X shapes data sovereignty, and Open Services Cloud frames technology sovereignty, both combined provides digital sovereignty. In details, it means: GAIA-X focuses on portability of cloud dataOpen Services Cloud focuses on portability of cloud data processing services and also fulfills the Data Act's requirements for cloud services portabilityYou need to have both portability of data and its associated data processing service to achieve digital sovereignty. One without the other has limited value.We implement GAIA-X data portability as services on top of Open Services Cloud to bring together these two features, **data portability and data processing portability££.","keywords":"","version":null},{"title":"Agent","type":0,"sectionRef":"#","url":"/xpanse/docs/agent","content":"","keywords":"","version":"Next"},{"title":"Agent as System Service​","type":1,"pageTitle":"Agent","url":"/xpanse/docs/agent#agent-as-system-service","content":" It's recommended to install the xpanse agent as a system service on the compute node so that the agent is always started.  Example of how to do it can be found here.  ","version":"Next","tagName":"h2"},{"title":"Supported Use cases​","type":1,"pageTitle":"Agent","url":"/xpanse/docs/agent#supported-use-cases","content":" The following use cases are currently supported by the agent.  ","version":"Next","tagName":"h2"},{"title":"Update Application configuration​","type":1,"pageTitle":"Agent","url":"/xpanse/docs/agent#update-application-configuration","content":" The application configuration management is based on Ansible. Agent wraps the GIT and Ansible client to execute the whole process.    Agent for containerized solutions The same agent can also be used for containerized solutions by deploying it in the same network where the other containers are reachable. In case of Kubernetes, it can be in the same POD as the application container.  ","version":"Next","tagName":"h3"},{"title":"Private APIs​","type":1,"pageTitle":"Agent","url":"/xpanse/docs/agent#private-apis","content":" The polling APIs in the xpanse are considered as private and shouldn't be reachable outside cloud provider's management area. ","version":"Next","tagName":"h2"},{"title":"Introduction to Open Services Cloud","type":0,"sectionRef":"#","url":"/xpanse/blog/2022/07/14/introduction","content":"Welcome to Eclipse Open Services Cloud. Today, the ecosystem is being locked out of public clouds. Only the cloud providers can create native managed services. The consequence is that it's pretty hard to integrate third party SaaS islands. On the other hand, the enterprises fear lock-in. There's no real service portability between different clouds. And even worse, cloud data is the ultimate lock-out and lock-in. The data lock-out means cloud data storage software is inefficient compare to cloud data storage service. Even if we can have a software storage on a purchased virtual machine, it's inefficient and expensive compared to native cloud storage services. The data lock-in is because the cloud data is tightly coupled to proprietary cloud services. It's hard to extract/export raw data from one cloud to another. Even if cloud data can be exported, it's not really useful without the accompanying cloud service. Open Services Cloud solves: federated: the goal is to create an open services ecosystem that works across different clouds. Open Services Cloud solves ecosystem lockout &amp; lack of service portability.open: the purpose is to enable customers to choose the services and providers of their choice. Thanks to that, we avoid customer lock-in.data infrastructure based on European values: Open Services Cloud enables European cloud providers and ecosystems to meet the needs of Europeans. Open Services Cloud open up the market, avoiding the hyperscalers to control and set the market for cloud and data services in Europe. Open Services Cloud is an Open Source initiative reshaping the cloud market. Welcome to Open Services Cloud.","keywords":"","version":null},{"title":"API","type":0,"sectionRef":"#","url":"/xpanse/docs/api","content":"","keywords":"","version":"Next"},{"title":"Swagger Hub​","type":1,"pageTitle":"API","url":"/xpanse/docs/api#swagger-hub","content":" You can interact with the xpanse API using the REST API. ","version":"Next","tagName":"h2"},{"title":"Architecture","type":0,"sectionRef":"#","url":"/xpanse/docs/architecture","content":"","keywords":"","version":"Next"},{"title":"Runtime​","type":1,"pageTitle":"Architecture","url":"/xpanse/docs/architecture#runtime","content":" The xpanse runtime is the glue between all components. It's a spring-boot application that bundles all the modules mentioned above into an executable jar.  ","version":"Next","tagName":"h2"},{"title":"xpanse Stack​","type":1,"pageTitle":"Architecture","url":"/xpanse/docs/architecture#xpanse-stack","content":" The xpanse runtime also needs other systems for it to fully function in a production setup.  Component\tDescriptionDatabase\tRDBMS based persistence layer terra-boot\tAsynchronous Terraform deployer tofu-maker\tAsynchronous OpenTofu deployer policy-man\tOPA based policy validation engine zitadel\tAuthentication and authorization UI\tReactJS based user interface Redis Cache\tUsed for all caching functionalities   ","version":"Next","tagName":"h2"},{"title":"Cloud Provider Credentials","type":0,"sectionRef":"#","url":"/xpanse/docs/cloud-provider-credentials","content":"","keywords":"","version":"Next"},{"title":"Authentication Capabilities​","type":1,"pageTitle":"Cloud Provider Credentials","url":"/xpanse/docs/cloud-provider-credentials#authentication-capabilities","content":" Each cloud provider allows different ways for its clients to authenticate itself and consume its API. For example, the same CSP might support username/password, API key, Oauth2 etc., as possible ways to connect to its API.  While xpanse plugins are responsible for integrating with the cloud provider APIs, it might not have implemented all possible ways to authenticate itself to the cloud provider API.  Every xpanse Plugin will define the authentication methods it supports to connect to its underlying cloud provider. This configuration is added and exposed by implementing the AuthenticationCapabilitiesinterface by the corresponding plugin.  The same information is also made available to consumers via the REST API method getCredentialCapabilitiesByCspin the CredentialsApi, which returns all credential types defined and supported by the plugin.  This list is enhanced as and when xpanse plugin for a specific CSP adds integration for new authentication methods supported by the CSP APIs.  ","version":"Next","tagName":"h2"},{"title":"Credential Configuration​","type":1,"pageTitle":"Cloud Provider Credentials","url":"/xpanse/docs/cloud-provider-credentials#credential-configuration","content":" Each credential configuration defined by a plugin contains the following information  Credential Type: variables, username/password, API key, Oauth2 etc.Dependent configuration: In case of Variables, the variable names and its values that must be provided so that the plugin can connect to the respective cloud provider.  ","version":"Next","tagName":"h3"},{"title":"Credential Type - Variables​","type":1,"pageTitle":"Cloud Provider Credentials","url":"/xpanse/docs/cloud-provider-credentials#credential-type---variables","content":" This is the only type that's currently implemented in xpanse. This will be extended in the near future.  Credential Variable Value Sources​  After a plugin has defined credential Type as variables and its respective variable names, then the consumer of this cloud provider, then values for these variable names must be provided, and this is possible to be done from different variable sources.  This is necessary to allow all combinations where some credential variable values are fixed for an environment and some can be different for each user. For example, in the case of Openstack plugin, the Identity URL might be fixed for each xpanse instance, but the username and password can vary for each xpanse user.  xpanse User Specific Credentials​  A credential for combination of a specific CSP, CredentialType and a specific xpanse user can be added using the addCredential API method. This means for all services ordered by a xpanse user or for monitoring services which are ordered by this specific xpanse user, this added credential will be used.  This exact request body definition for each of the CSP and Credential Types can be found using getCredentialOpenApi. This method provides a link to OpenAPI definition and sample request for adding credentials for each CSP.  Credential variable values sent from this API are stored directly into CredentialCache.  OS Environment Variables​  Based on the CredentialDefinition configured in the Plugins, the xpanse runtime can be started by injecting default credential configuration variables as OS environment variables to the runtime. If values for all credential variables aren't available from other sources, then the runtime will search for credential variables from OS environment variables.  Please note that the variables from OS environment values are read as-is. It's therefore necessary to keep credential variable names unique even between two different credential types of the same CSP.  Credential Variables &quot;Joining&quot;​  As described in the above sections, for each credential type, a certain set of variables are defined. Values for allof these variables are needed for the credential to be valid.  We join credential variable values from all the sources mentioned in the above sectionsto get the final credential variable values that will be used for connecting to the CSP.  Credential values are read from different values and joined into one single set of all required credential variable values of a given type.  If the value of a credential variable is available in multiple sources, then the value coming from the source of higher priority is used.  Service Deployment​  During the service deployment, all the values for the variables defined in the CredentialDefinition is derived from different sources in a priority sequence provided below  CredentialType and variables from deploy variables. As part of the registering services, the service provider can define what type of credentials is needed for ordering the service. The end user must add this credential using addCredential method. This will add credentials to CredentialsCache. Credential variable values from OS environment variables.    Service Monitoring​  During the service monitoring, all the values for the variables defined in the CredentialDefinition is derived from different sources in a priority sequence provided below  Credentials available in the CredentialsCache.credential variables from OS environment variables.    ","version":"Next","tagName":"h3"},{"title":"Credential Cache​","type":1,"pageTitle":"Cloud Provider Credentials","url":"/xpanse/docs/cloud-provider-credentials#credential-cache","content":" Credential cache is the credential store in xpanse runtime which stores all credential data which are required to be used in the later phase of the workflows. Credentials provided by the API are stored as an in-memory cache map.  ","version":"Next","tagName":"h2"},{"title":"Cache Eviction​","type":1,"pageTitle":"Cloud Provider Credentials","url":"/xpanse/docs/cloud-provider-credentials#cache-eviction","content":" Credentials are evicted from cache after the provided expiry time or by default after 1 hour.  ","version":"Next","tagName":"h3"},{"title":"Data Encryption​","type":1,"pageTitle":"Cloud Provider Credentials","url":"/xpanse/docs/cloud-provider-credentials#data-encryption","content":" When the runtime is started with an AES secure key, then all sensitive data is encrypted and stored in the JVM. More information can be found here.  ","version":"Next","tagName":"h3"},{"title":"Multiple Credentials Found​","type":1,"pageTitle":"Cloud Provider Credentials","url":"/xpanse/docs/cloud-provider-credentials#multiple-credentials-found","content":" When multiple credentials are found in the credential cache for the same xpanse user, credential type, and CSP, then we simply use the first available credential. ","version":"Next","tagName":"h3"},{"title":"Authentication and Authorization","type":0,"sectionRef":"#","url":"/xpanse/docs/authentication-authorization","content":"","keywords":"","version":"Next"},{"title":"OIDC​","type":1,"pageTitle":"Authentication and Authorization","url":"/xpanse/docs/authentication-authorization#oidc","content":" Authentication and Authorization are implemented using OIDC (OpenID Connect). This helps us to outsource the security layer to an external Identity Provider and thereby not storing any user data within xpanse.  ","version":"Next","tagName":"h2"},{"title":"OIDC Providers and its Configuration​","type":1,"pageTitle":"Authentication and Authorization","url":"/xpanse/docs/authentication-authorization#oidc-providers-and-its-configuration","content":" Similar to other integrations provided in xpanse, the runtime can be extended to be integrated with any Identity Provider that supports OIDC standards. It's not limited to any specific product.  Any OIDC provider can't be used directly out of the box. It needs a variety of configurations on the provider. From xpanse project, we aim to provide 'Infrastructure as Code' which will automate all required configuration on the Identity provider.  All Identity provider related configuration on consumers can only be obtained only after the OIDC provider is fully configured with the consumer detail  ","version":"Next","tagName":"h3"},{"title":"Zitadel​","type":1,"pageTitle":"Authentication and Authorization","url":"/xpanse/docs/authentication-authorization#zitadel","content":" As a default implementation, xpanse runtime is integrated with Zitadel Identity Provider.  To enable Zitadel authentication layer, the runtime must be started with zitadel profile.  ","version":"Next","tagName":"h2"},{"title":"Zitadel Installation​","type":1,"pageTitle":"Authentication and Authorization","url":"/xpanse/docs/authentication-authorization#zitadel-installation","content":" Zitadel instance for local development can be setup using the steps mentioned here..  An example for setting up production like Zitadel instance can be found in our testbed installation steps here..  ","version":"Next","tagName":"h3"},{"title":"Zitadel Provider Configuration​","type":1,"pageTitle":"Authentication and Authorization","url":"/xpanse/docs/authentication-authorization#zitadel-provider-configuration","content":" After starting the vanilla Zitadel application, all required configurations can be applied using the Terraform scripts provided. All steps required for initial configuration are available here.  ","version":"Next","tagName":"h3"},{"title":"Zitadel Consumer Configuration​","type":1,"pageTitle":"Authentication and Authorization","url":"/xpanse/docs/authentication-authorization#zitadel-consumer-configuration","content":" All generated configuration details can be obtained as documented here.  Runtime​  Runtime application needs the following properties to be set for the authentication and authorization to work.  - authorization.server.endpoint - authorization.api.client.id - authorization.api.client.secret - authorization.swagger.ui.client.id   UI​  UI can be then started using the consumer details as documented here.  End Users Configuration​  At the moment, it's only feasible to create user only via the Zitadel UI as we don't have SMTP available which is needed for self-registration of users.  Admin Users​  By default, Zitadel provides a root user using which other users can be created and also made other administrators. Only these users can add other end-users on Zitadel.  ","version":"Next","tagName":"h3"},{"title":"Role Based Access Control - RBAC​","type":1,"pageTitle":"Authentication and Authorization","url":"/xpanse/docs/authentication-authorization#role-based-access-control---rbac","content":" We use OIDC providers to also implement RBAC on xpanse runtime API and also in UI.  ","version":"Next","tagName":"h2"},{"title":"Roles​","type":1,"pageTitle":"Authentication and Authorization","url":"/xpanse/docs/authentication-authorization#roles","content":" To keep our initial role configuration simple, the following roles are considered  user - this is the role allocated to end user. isv - this is the role allocated to ISV (Independent Software Vendor) users. csp - this is the role allocated to CSP (Cloud Service Provider) users. admin - this is the role allocated to administrators running xpanse. Default Role - When no roles are assigned to a user, then every registered is by default assumed to have the user role.  ","version":"Next","tagName":"h3"},{"title":"Role Assignment​","type":1,"pageTitle":"Authentication and Authorization","url":"/xpanse/docs/authentication-authorization#role-assignment","content":" After the end users are registered/created on Zitadel, the administrator must assign roles to the user.  This option can be found on the UI under - users -&gt; click on user -&gt; Authorizations -&gt; + New  ","version":"Next","tagName":"h3"},{"title":"Role Validation​","type":1,"pageTitle":"Authentication and Authorization","url":"/xpanse/docs/authentication-authorization#role-validation","content":" Each API method is configured to be allowed only for specific roles. Spring Security fetches the user information from Identity Provider and based on the roles allocated to the user, we decide if the user is allowed to access an API or not. If the user isn't authorized to access, then API returns HTTP code 403.xpanse UI fetches the user information from Identity Provider and based on the roles allocated to the user, menu options are displayed in the UI.  ","version":"Next","tagName":"h3"},{"title":"Attribute Based Access Control - ABAC​","type":1,"pageTitle":"Authentication and Authorization","url":"/xpanse/docs/authentication-authorization#attribute-based-access-control---abac","content":" ","version":"Next","tagName":"h2"},{"title":"OIDC Metadata​","type":1,"pageTitle":"Authentication and Authorization","url":"/xpanse/docs/authentication-authorization#oidc-metadata","content":" Metadata for ISVs​  In order to allow service vendors to manage only their own services, we use ABAC. This is achieved by using metadata feature of OIDC to assign isv users to a specific ISV value.    While registering a service, the service vendor must specify the same ISV value in the isv field. With this, users of another ISV won't be allowed to view, update or delete these service templates.  Metadata for CSPs​  Also for CSP users, the metadata must be used to define the CSP to which the user belongs to.    ","version":"Next","tagName":"h3"},{"title":"Execute authenticated APIs​","type":1,"pageTitle":"Authentication and Authorization","url":"/xpanse/docs/authentication-authorization#execute-authenticated-apis","content":" When the application has activated authorization with oauth profile, all protected APIs must have anAuthorization header in the Bearer ${access token} format in the HTTP request. Here are two ways to get access_token for executing authenticated APIs:  The first way is to open the Swagger-UI page of the xpanse server. In case of local development server, the Swagger-UI is reachable on http://localhost:8080/swagger-ui/index.html.  Click on the 'Authorize' button on the SwaggerUI page to open the authentication window. Click on the 'select all' option to select all 'Scopes' and click on 'Authorize'  The browser will redirect to the login page of the IAM service. Fill username and password to complete user login. Once the login is successful, the control is automatically redirected to the Swagger-UI page. Close the authentication window and select the API which you want to execute and click to expand it, then click on 'Try it out' and 'Execute' to execute the API method. In the Curl command, you can see the request header named Authorization with the value of${access_token} after prefix Bearer .  The other way is to use the Authorize REST API. Call the method using browser. In case of local development server, the URL is http://localhost:8080/auth/authorize. The browser will redirect to the login page of the IAM service. Fill username and password to complete user login. After successful login in the IAM, the browser will back to the token API URL with response model 'TokenResponse' with field 'access_token.' Then you can use the value of 'access_token' to fill header 'Authorization' in the HTTP request when executing authenticated APIs with CLI or the other HTTP client tools.  ","version":"Next","tagName":"h2"},{"title":"Disable Authentication and Authorization​","type":1,"pageTitle":"Authentication and Authorization","url":"/xpanse/docs/authentication-authorization#disable-authentication-and-authorization","content":" If we want to disable Authentication and Authorization completely, then start the application with profile noauth.  If we want to disable only Authorization (RBAC), then we must start the application with configuration enable.role.protection=false  This might be necessary when we plan to run xpanse behind any other applications which will take care of authentication and authorization. ","version":"Next","tagName":"h2"},{"title":"Configuration Language","type":0,"sectionRef":"#","url":"/xpanse/docs/configuration-language","content":"","keywords":"","version":"Next"},{"title":"Deployment Scripts​","type":1,"pageTitle":"Configuration Language","url":"/xpanse/docs/configuration-language#deployment-scripts","content":" In XSDL, the deployer can contain the script that must be executed for provisioning the managed service. Currently, the only allowed script is Terraform.  ","version":"Next","tagName":"h3"},{"title":"Flavors​","type":1,"pageTitle":"Configuration Language","url":"/xpanse/docs/configuration-language#flavors","content":" For each managed service, we can define different flavors of it. For example, different sizes of the VM, etc. End user can then select the flavor of their preference for the service while ordering.  ","version":"Next","tagName":"h3"},{"title":"Flavor properties​","type":1,"pageTitle":"Configuration Language","url":"/xpanse/docs/configuration-language#flavor-properties","content":" Flavors can have properties which can be simply declared and referred in the deployment script too with the same property names. Runtime will ensure that these variables are automatically available for the deployment scripts  ","version":"Next","tagName":"h3"},{"title":"Deployment Variables​","type":1,"pageTitle":"Configuration Language","url":"/xpanse/docs/configuration-language#deployment-variables","content":" As part of the XSDL, the managed service provider can define variables that can be either entered by the user or available as defaults or variables that can be read for any other sources such as the environment variables. All possible types of variables are defined here Deployment VariablesThe variables can then be used in the deployment scripts.  ","version":"Next","tagName":"h3"},{"title":"Availability Zones​","type":1,"pageTitle":"Configuration Language","url":"/xpanse/docs/configuration-language#availability-zones","content":" XSDL provides a way to define if the service supports deploying on availability zones which the end user wishes.  Or even if the service needs the end user to select multiple availability zones, even that's possible. The availability zone information can be directly injected into deployment scripts.  ","version":"Next","tagName":"h3"},{"title":"ISV Contact Details​","type":1,"pageTitle":"Configuration Language","url":"/xpanse/docs/configuration-language#isv-contact-details","content":" Each ISV can provide their support contacts as part of XSDL. This can then be used by end user or the CSP in case of issues with the service deployment or managing the service after deployment.  ","version":"Next","tagName":"h3"},{"title":"Register Service templates​","type":1,"pageTitle":"Configuration Language","url":"/xpanse/docs/configuration-language#register-service-templates","content":" xpanse provides different options to register service templates defined using XSDL:  REST API on the xpanse runtimexpanse UI  ","version":"Next","tagName":"h2"},{"title":"Service Template Registration Sequence​","type":1,"pageTitle":"Configuration Language","url":"/xpanse/docs/configuration-language#service-template-registration-sequence","content":" ","version":"Next","tagName":"h3"},{"title":"Caching","type":0,"sectionRef":"#","url":"/xpanse/docs/caching","content":"","keywords":"","version":"Next"},{"title":"Development Environments​","type":1,"pageTitle":"Caching","url":"/xpanse/docs/caching#development-environments","content":" For all development purposes, we use a local memory-based cache Caffeine Cache which is automatically created during the server startup. And when the service stops, all data in the local cache will be automatically cleared.  Caffeine Cache is only for development environments The Caffeine Cache is purely for development and test purposes only and must be avoided for production installation of xpanse.  ","version":"Next","tagName":"h2"},{"title":"Production Environments​","type":1,"pageTitle":"Caching","url":"/xpanse/docs/caching#production-environments","content":" For production environments, we use a distributed cache. The below distributed caches are at the moment fully supported. By support, we mean configurations for the cache added and also all functionalities tested.  ","version":"Next","tagName":"h2"},{"title":"Redis Cache​","type":1,"pageTitle":"Caching","url":"/xpanse/docs/caching#redis-cache","content":" At the moment, this is the only distributed cache that's fully supported. To use Redis as the cache manager for xpanse, it must be activated by starting the application with profile redis and by replacing the placeholders for redis host, port and password as below.  java -jar xpanse-runtime-*-SNAPSHOT.jar \\ -Dspring.profiles.active=redis \\ -Dspring.data.redis.host=${redis-host} \\ -Dspring.data.redis.port=${redis-port} \\ -Dspring.data.redis.password=${redis-password}   Versions Supported​  Current supported version of Redis is 7.4.2. This is the latest LTS release of Redis.  Default Configuration​  The default configuration parameters for redis profile can be found  here  .  Overriding Default Configuration​  We can override the above three default configurations by starting the application as below by replacing the placeholders with actual values.  java -jar xpanse-runtime-*-SNAPSHOT.jar \\ -Dspring.profiles.active=redis \\ -Dspring.data.redis.host=${replace-with-redis-host} \\ -Dspring.data.redis.port=${replace-with-redis-port} \\ -Dspring.data.redis.password=${replace-with-redis-password}   secrets better as environment variables It's safe to provide the redis-related properties as environment variables rather than passing them directly in the command line. In case of this, the same property name must be set in UPPERCASE and underscore separated instead of dot for all variables.  network between xpanse and its redis Using this startup command, the redis can run on any machine that's reachable from the xpanse runtime application.  Redis as a Docker Container​  Redis offers official Docker images for running a database as a container. More details can be found here on the official page of Redis website here and on DockerHub here.  Starting new container​  While starting the Redis docker container for the first time, we can configure redis-port, andredis-password as below and the same can be used in starting the xpanse runtime using the command described  above.  docker pull redis:7.4.2   By starting the container with the below command, the redis is started by automatically configuring the redis with xpanse redis port and password.  docker run --name ${container-name} \\ -e REDIS_PASSWORD=${replace-with-redis-password} -p &lt;replace-with-redis-port&gt;:6379 -d redis:7.4.2   Avoid secrets in command line To avoid passing database related properties in command line, we can use the --env-file option of the docker run command to store all sensitive data.  ","version":"Next","tagName":"h3"},{"title":"Cached Data​","type":1,"pageTitle":"Caching","url":"/xpanse/docs/caching#cached-data","content":" The application will cache multiple types of data. Each type of data cache will be registered into a cache manager with a specific name and configuration. There are two types of cache managers, one is for Caffeine Cache and the other is for Redis Cache.  Cache manager for Caffeine defined in the class CaffeineCacheConfig  here  Cache manager for Redis defined in the class RedisCacheConfig  here  In both cache managers, the following caches are created with the following names:  REGION_AZS_CACHE_NAME -- to cache different available zones of the regions in the service cloud providers. SERVICE_FLAVOR_PRICE_CACHE_NAME -- to cache different prices of the flavors of the service with different billing models in the service cloud providers. CREDENTIAL_CACHE_NAME -- to cache different credentials the service cloud providers provided by the users. MONITOR_METRICS_CACHE_NAME --to cache different monitor metrics of the deployed services. DEPLOYER_VERSIONS_CACHE_NAME --to cache available versions of the deployer tools, such as terraform, opentofu etc. ","version":"Next","tagName":"h2"},{"title":"Bug Handling Process","type":0,"sectionRef":"#","url":"/xpanse/docs/Contribute/bug-handling-process","content":"","keywords":"","version":"Next"},{"title":"Overview​","type":1,"pageTitle":"Bug Handling Process","url":"/xpanse/docs/Contribute/bug-handling-process#overview","content":" xpanse is aiming to build a secure system from the foundation, applying the best industry practices in terms of development quality. However, as in every software projects, bugs do happen. This process explains how we handle bugs.  ","version":"Next","tagName":"h2"},{"title":"How to Report a Bug?​","type":1,"pageTitle":"Bug Handling Process","url":"/xpanse/docs/Contribute/bug-handling-process#how-to-report-a-bug","content":" If you think you have found a bug in xpanse, please open an issue in GitHub and in the project that you think is the source of the issue. Use the provided template:  What's the action to reproduce the bug? (Steps to reproduce)What's the result you see? (Actual result)What's the result you expect? (Expected behaviour)Frequency? (always, sometimes, one-time issue)Tested version (Release or commit version, platform)Do you know any workaround of this issue? (link to workaround/mitigation steps etc)Do you have a fix for this issue?  Developers review the reported issues and perform triage (see below). When a fix is available, the ticket is updated with the details of the solution.  ","version":"Next","tagName":"h2"},{"title":"Bug Triage​","type":1,"pageTitle":"Bug Handling Process","url":"/xpanse/docs/Contribute/bug-handling-process#bug-triage","content":" The bug triage is a process where developers asses the bug and set its severity and domain. At the end of this process the bug will:  Be classified as a security issue, normal bug, feature request, or be rejected if the feature is working as planned or couldn't be reproduced.Have its severity set. Please refer to the documentation of severity levels below.  If the bug is classified as a security vulnerability, please also refer to our security guidelines here.  If the bug is confirmed as a bug, the developer will assign bug severity: critical, major, minor or low.  Note: Critical severity bugs make a feature unusable, cause a major data loss. There is no workaround, or a complex one. Normal severity bugs make a feature hard to use, but there is a workaround (including another feature to use instead of the desired one). Minor severity bugs cause a loss of non-critical feature (like missing or incorrect logging). Low severity bugs cause minor inconveniences (like a typo in the user interface or in the documentation).  The bug can originate in the software developed by the project, or from the dependencies we use from other sources. The process of handling a bug report will change between those two cases:  ","version":"Next","tagName":"h2"},{"title":"When the Issue is in the Code Developed by the Project`​","type":1,"pageTitle":"Bug Handling Process","url":"/xpanse/docs/Contribute/bug-handling-process#when-the-issue-is-in-the-code-developed-by-the-project","content":" In the case where the bug originates in the code directly maintained by the Project, the bug is handled directly in the bug tracker.  ","version":"Next","tagName":"h3"},{"title":"When the Issue Originates from Dependencies​","type":1,"pageTitle":"Bug Handling Process","url":"/xpanse/docs/Contribute/bug-handling-process#when-the-issue-originates-from-dependencies","content":" If the issue was identified in dependencies code, we report an issue in a way appropriate to that particular dependency project. We store the reference to their bug report in our bug tracker. Depending on the bug severity, we might decide to develop and maintain a fix locally. However, we strongly prefer the dependency library maintainers to fix the issue first, and then get it with a regular dependency version update.  Please note also that we periodically update maintained dependencies, regardless of the bugs filled in our system. Our goal is to update to the latest stable version of the project.  ","version":"Next","tagName":"h3"},{"title":"Detailed Workflow​","type":1,"pageTitle":"Bug Handling Process","url":"/xpanse/docs/Contribute/bug-handling-process#detailed-workflow","content":" ","version":"Next","tagName":"h2"},{"title":"Bug Sources​","type":1,"pageTitle":"Bug Handling Process","url":"/xpanse/docs/Contribute/bug-handling-process#bug-sources","content":" Bugs might be reported by different sources, including Project's own findings (like QA), partner findings or community. There might be also different ways the Project team learns about the issue, including chats, discussion forums etc. Issues coming from different sources are centralized in the bug tracker, which also provides a unified identification of all issues.  ","version":"Next","tagName":"h3"},{"title":"Acknowledgement and Bug Triage​","type":1,"pageTitle":"Bug Handling Process","url":"/xpanse/docs/Contribute/bug-handling-process#acknowledgement-and-bug-triage","content":" After the bug is entered, a developer will perform triage. The process starts from acknowledging the issue and then consists of verifying all the information provided by the bug reporter to reproduce the issue. The developer performing triage might ask additional questions. Then they assign severity and domain to the issue in the bug tracker. They also check which versions are affected and might modify the severity level set by the reporter. Any project member, or the bug reporter, who disagrees with the assignment might comment on the issue.  If there is a fix available from the reporter, the developer also verifies if the fix is correct and matches the IP policy. If the fix is judged acceptable, the process might skip to the Releasing step.  We aim at the first answer of the triage (either finishing triage, or additional questions to the reporter) in three working days for critical bugs and seven days for other bugs. In case of a critical bug, the person performing triage informs the maintainers of the affected subsystem.  ","version":"Next","tagName":"h3"},{"title":"Prioritizing and Fixing​","type":1,"pageTitle":"Bug Handling Process","url":"/xpanse/docs/Contribute/bug-handling-process#prioritizing-and-fixing","content":" Bugs with the severity attached enter the prioritization process. It includes a weekly meeting when the team reviews bugs entered or modified during the last week: those during the process of triage, and those with the triage finished. For the bugs with triage finished, the team sets the priority and might assign it to a developer.  The bug fixes should follow the same contributions guidelines as any other contribution. The best practice is to develop a fix for the bug in a separate branch. Fixes for related bugs are possible in the same branch.  ","version":"Next","tagName":"h3"},{"title":"Releasing​","type":1,"pageTitle":"Bug Handling Process","url":"/xpanse/docs/Contribute/bug-handling-process#releasing","content":" When a bug fix is available in a branch, the developer creates a merge request. When the change is accepted, it's merged in the main branch. The developer in charge of the bug verifies with the release manager to which branches the change should be backported.  If the bug comes from a dependent library, developers upstream the bug fix. If the upstream is delayed, the Project might ship a local fix. However, we aim to upstream all fixes.  During the time of development of the patch and eventual upstream, the developer updates the documentation (if appropriate), and adds a notification to the release notes. Our release notes contain: links to bugs fixed in the release, links to CVEs fixed in the release (publicly known) and a list of CVEs fixed that are still under embargo.  If the bug is classified as critical, it might be decided to perform a separate bug-fix release to fix the issue. Otherwise, the bug fix lands in the next bug-fix release. ","version":"Next","tagName":"h3"},{"title":"New Developers","type":0,"sectionRef":"#","url":"/xpanse/docs/Contribute/new-developers","content":"","keywords":"","version":"Next"},{"title":"Creating an account on Eclipse Foundation​","type":1,"pageTitle":"New Developers","url":"/xpanse/docs/Contribute/new-developers#creating-an-account-on-eclipse-foundation","content":" Head to the Eclipse foundation websiteand set up an account by entering your:  EmailUsernameFull nameOrganizationPasswordCountry  Then read and check the box to agree to Terms of Use, Privacy Policy and Code of Conduct. When you complete that, follow the instructions sent to your email to activate the account.  ","version":"Next","tagName":"h2"},{"title":"Signing the ECA​","type":1,"pageTitle":"New Developers","url":"/xpanse/docs/Contribute/new-developers#signing-the-eca","content":" In order to contribute to the eclipse-xpanse, you need to sign the  Eclipse Contributor Agreement  which describes the terms under which you can contribute to the project.  If you sign this ECA, you confirm your legal rights to submit the code to the project. You also provide license to your contributions to Eclipse and specified users, however you still own your contributions.  ","version":"Next","tagName":"h2"},{"title":"GitHub user​","type":1,"pageTitle":"New Developers","url":"/xpanse/docs/Contribute/new-developers#github-user","content":" Since xpanse uses GitHub as its code repository, Developer's GitHub user ID must be configured in Eclipse Foundation Account. Without this any contributions can't be merged. . Steps are documented here.  ","version":"Next","tagName":"h2"},{"title":"Eclipse Foundation Contributor​","type":1,"pageTitle":"New Developers","url":"/xpanse/docs/Contribute/new-developers#eclipse-foundation-contributor","content":" Contributor guidelines from Eclipse foundation can be additionally found here. ","version":"Next","tagName":"h2"},{"title":"Calendar","type":0,"sectionRef":"#","url":"/xpanse/docs/Contribute/calendar","content":"Calendar Our meetings are publicly announced on our Google calendar and the community is welcome to join them. You can navigate either through the calendar below or subscribe from your email client using our ICS File.","keywords":"","version":"Next"},{"title":"Code of conduct","type":0,"sectionRef":"#","url":"/xpanse/docs/Contribute/code-of-conduct","content":"","keywords":"","version":"Next"},{"title":"Our pledge​","type":1,"pageTitle":"Code of conduct","url":"/xpanse/docs/Contribute/code-of-conduct#our-pledge","content":" In the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to making participation in our collective and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation.  ","version":"Next","tagName":"h2"},{"title":"Our standards​","type":1,"pageTitle":"Code of conduct","url":"/xpanse/docs/Contribute/code-of-conduct#our-standards","content":" Examples of behavior that contributes to creating a positive environment include:  Using welcoming and inclusive languageBeing respectful of differing viewpoints and experiencesGracefully accepting constructive criticismFocusing on what's best for the communityShowing empathy towards other community members  Examples of unacceptable behavior by participants include:  The use of sexualized language or imagery and unwelcome sexual attention or advancesTrolling, insulting/derogatory comments, and personal or political attacksPublic or private harassmentPublishing others’ private information, such as a physical or electronic address, without explicit permissionOther conduct which could reasonably be considered inappropriate in a professional setting  ","version":"Next","tagName":"h2"},{"title":"Our responsibilities​","type":1,"pageTitle":"Code of conduct","url":"/xpanse/docs/Contribute/code-of-conduct#our-responsibilities","content":" Maintainers are responsible for clarifying the standards of acceptable behavior and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behavior.  Maintainers have the right and responsibility to edit, delete, hide, or lock code, comments, commits, edits, issues, posts, pull requests, and other contributions that aren't aligned to this code of conduct, or to ban temporarily or permanently any contributor for other behaviors that they deem inappropriate, threatening, offensive, or harmful.  ","version":"Next","tagName":"h2"},{"title":"Scope​","type":1,"pageTitle":"Code of conduct","url":"/xpanse/docs/Contribute/code-of-conduct#scope","content":" This code of conduct applies both within collective spaces and in public spaces when an individual is representing the collective or its community.  Examples of representing a collective or community include using an official collective email address, posting via an official social media account, or acting as an appointed representative at an online or offline event.  Representation of the collective may be further defined and clarified by maintainers.  ","version":"Next","tagName":"h2"},{"title":"Enforcement​","type":1,"pageTitle":"Code of conduct","url":"/xpanse/docs/Contribute/code-of-conduct#enforcement","content":" Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting the project team.  All complaints will be reviewed and investigated and will result in a response that's deemed necessary and appropriate to the circumstances. The project team is obligated to maintain confidentiality with regard to the reporter of an incident. Further details of specific enforcement policies may be posted separately.  Project maintainers who don't follow or enforce the Code of Conduct in good faith may face temporary or permanent repercussions as determined by other members of the project’s leadership.  ","version":"Next","tagName":"h2"},{"title":"Attribution​","type":1,"pageTitle":"Code of conduct","url":"/xpanse/docs/Contribute/code-of-conduct#attribution","content":" This code of conduct is adapted from the Contributor Covenant, version 1.4, available at https://www.contributor-covenant.org/version/1/4/code-of-conduct.html  For answers to common questions about this code of conduct, see https://www.contributor-covenant.org/faq ","version":"Next","tagName":"h2"},{"title":"Continuous Integration","type":0,"sectionRef":"#","url":"/xpanse/docs/Contribute/continuous-integration","content":"","keywords":"","version":"Next"},{"title":"xpanse Repository​","type":1,"pageTitle":"Continuous Integration","url":"/xpanse/docs/Contribute/continuous-integration#xpanse-repository","content":" The xpanse repository contains the main application for xpanse project and is written in Java.  CI pipeline for this repository is defined here as GitHub actions. This Job builds complete project, runs all tests and also checks for the code format.  ","version":"Next","tagName":"h3"},{"title":"xpanse UI Repository​","type":1,"pageTitle":"Continuous Integration","url":"/xpanse/docs/Contribute/continuous-integration#xpanse-ui-repository","content":" The xpanse UI repository contains the frontend application for xpanse project and is written in TypeScript and based on ReactJS library.  CI pipeline for this repository is defined here as GitHub actions. This Job runs the ES linter and prettier formatter checks.  ","version":"Next","tagName":"h3"},{"title":"xpanse Website Repository​","type":1,"pageTitle":"Continuous Integration","url":"/xpanse/docs/Contribute/continuous-integration#xpanse-website-repository","content":" The xpanse website repository contains the documentation for xpanse project and is written in TypeScript and based on Docusaurus library.  CI pipeline for this repository is defined here as GitHub actions. This Job runs the ES linter and prettier formatter checks.  ","version":"Next","tagName":"h3"},{"title":"Pull Requests and CI Pipelines​","type":1,"pageTitle":"Continuous Integration","url":"/xpanse/docs/Contribute/continuous-integration#pull-requests-and-ci-pipelines","content":" All PRs on the respective repositories can be merged only when the underlying CI pipeline is successful. ","version":"Next","tagName":"h2"},{"title":"Planning","type":0,"sectionRef":"#","url":"/xpanse/docs/Contribute/planning","content":"","keywords":"","version":"Next"},{"title":"Agile Development Process​","type":1,"pageTitle":"Planning","url":"/xpanse/docs/Contribute/planning#agile-development-process","content":" We follow Agile/Scrum development process. Each sprint is 2 weeks long.  ","version":"Next","tagName":"h3"},{"title":"Sprint Planning​","type":1,"pageTitle":"Planning","url":"/xpanse/docs/Contribute/planning#sprint-planning","content":" Sprint Planning meeting happens every alternate Mondays.GitHub Milestones is used for scheduling, planning and reporting sprint deliveries.All topics planned in sprints are only tracked as GitHub issues  ","version":"Next","tagName":"h3"},{"title":"Roadmap & Backlog​","type":1,"pageTitle":"Planning","url":"/xpanse/docs/Contribute/planning#roadmap--backlog","content":" At the moment, all open topics are documented in the GitHub Project. ","version":"Next","tagName":"h3"},{"title":"Pull Requests","type":0,"sectionRef":"#","url":"/xpanse/docs/Contribute/pull-requests","content":"","keywords":"","version":"Next"},{"title":"Things to remember​","type":1,"pageTitle":"Pull Requests","url":"/xpanse/docs/Contribute/pull-requests#things-to-remember","content":" Don't forget to periodically sync your fork with the upstream repository. How to sync a fork?Don't forget to periodically rebase your branch git rebase -i origin/mainIf you have a group of commits related to the same change, please squash your commits into one and force push to your branch.Test that your change works by adapting or adding tests.Follow the boy scout rule to &quot;Always leave the campground cleaner than you found it.&quot;Build your changes, In case of all Java applications, make sure you do a build before doing a PR. and the build has to be successful   $ mvn clean verify   In case of xpanse UI and website, make sure the application starts without any errors and warnings:   $ npm run start   If your PR has conflicts with the main then rebase the branch. PRs with conflicts are unlikely to be appliedDon't change too much in a PR. The smaller the PR the easier it's to review, apply and the faster it will be doneEven if we're monitoring closely the PR, if you think your PR doesn't move forward fast enough, don't hesitate to ping in a PR comment to get some update.  ","version":"Next","tagName":"h2"},{"title":"Running Linters​","type":1,"pageTitle":"Pull Requests","url":"/xpanse/docs/Contribute/pull-requests#running-linters","content":" To ensure best code practices and uniformity within the community, we've different static code checkers, linters and code formatters used. The PR GitHub actions will fail if any of these checks fail. Hence, the developer must run these checks locally once and fix the errors highlights before pushing changes to origin.  ","version":"Next","tagName":"h2"},{"title":"Java Applications​","type":1,"pageTitle":"Pull Requests","url":"/xpanse/docs/Contribute/pull-requests#java-applications","content":" Checkstyle.Spotless.  mvn spotless:check &amp;&amp; mvn checkstyle:check # If spotless reports any errors, they can be auto corrected with the below command. mvn spotless:apply # If checkstyle reports any errors, they must be manually fixed by the developer.   ","version":"Next","tagName":"h3"},{"title":"UI​","type":1,"pageTitle":"Pull Requests","url":"/xpanse/docs/Contribute/pull-requests#ui","content":" PrettierES Lint  # Prettier to automatically format all files. npx prettier --config .prettierrc --write . # ES Lint static checks will report errors if any and must be fixed manually. npx eslint . --max-warnings=0 # Typescript compiler will report any TypeScript errors and must be fixed manually. tsc   ","version":"Next","tagName":"h3"},{"title":"Website​","type":1,"pageTitle":"Pull Requests","url":"/xpanse/docs/Contribute/pull-requests#website","content":" In addition to the above linters mentioned in the UI, website uses vale linter for grammar and spell checks.  # Errors reported by vale linter must be fixed manually. docker run --rm -v $(pwd)/.github/vale/styles:/styles --pull=always --rm -v $(pwd):/docs -w /docs jdkato/vale .  ","version":"Next","tagName":"h3"},{"title":"Releases","type":0,"sectionRef":"#","url":"/xpanse/docs/Contribute/releases","content":"","keywords":"","version":"Next"},{"title":"xpanse Website​","type":1,"pageTitle":"Releases","url":"/xpanse/docs/Contribute/releases#xpanse-website","content":" We re-deploy the xpanse website for every commit that's done to the main branch.  Continuous deployment is implemented based on GitHub actions as configured here.  ","version":"Next","tagName":"h2"},{"title":"xpanse Stack​","type":1,"pageTitle":"Releases","url":"/xpanse/docs/Contribute/releases#xpanse-stack","content":" All application necessary to run the xpanse stack are released as docker images in the GitHub docker registry. Java applications are additionally released as jar files which can be downloaded directly from the GitHub release notes.  Docker images can be found at GitHub Packages. ","version":"Next","tagName":"h2"},{"title":"Security Policy","type":0,"sectionRef":"#","url":"/xpanse/docs/Contribute/security","content":"","keywords":"","version":"Next"},{"title":"Versions​","type":1,"pageTitle":"Security Policy","url":"/xpanse/docs/Contribute/security#versions","content":" The latest version of xpanse is supported.  ","version":"Next","tagName":"h2"},{"title":"Reporting a Vulnerability​","type":1,"pageTitle":"Security Policy","url":"/xpanse/docs/Contribute/security#reporting-a-vulnerability","content":" If you think you found a vulnerability, and even if you aren't sure about it, please report it right away by sending an email to: xpanse-dev. Please try to be as explicit as possible, describing all the steps and example code to reproduce the security issue.  We will review it thoroughly and get back to you as fast as possible.  ","version":"Next","tagName":"h2"},{"title":"Public Discussions​","type":1,"pageTitle":"Security Policy","url":"/xpanse/docs/Contribute/security#public-discussions","content":" Please restrain from publicly discussing a potential security vulnerability.  It's better to discuss privately and try to find a solution first, to limit the potential impact as much as possible.  ","version":"Next","tagName":"h2"},{"title":"Eclipse Foundation Guidelines​","type":1,"pageTitle":"Security Policy","url":"/xpanse/docs/Contribute/security#eclipse-foundation-guidelines","content":" Eclipse Foundation guidelines for reporting vulnerabilities can be found here. ","version":"Next","tagName":"h2"},{"title":"Database","type":0,"sectionRef":"#","url":"/xpanse/docs/database","content":"","keywords":"","version":"Next"},{"title":"Development Environments​","type":1,"pageTitle":"Database","url":"/xpanse/docs/database#development-environments","content":" For all development purposes, we use a file-based H2 database which is automatically created during the server startup.  ","version":"Next","tagName":"h2"},{"title":"Admin UI​","type":1,"pageTitle":"Database","url":"/xpanse/docs/database#admin-ui","content":" The UI to the H2 DB can be accessed at http://localhost:8080/h2-console/login.jsp. Credentials to this admin UI can be found here.  ","version":"Next","tagName":"h3"},{"title":"Cleanup​","type":1,"pageTitle":"Database","url":"/xpanse/docs/database#cleanup","content":" Since it's a file-based database, the entries aren't lost with reboot of the server.  To clean up the database, the database files must be simply deleted and the server must be restarted for new fresh DB files to be recreated. Execute the below command on the root of the folder xpanse project.  rm *.db   H2 is only for development environments The H2 database is purely for development and test purposes only and must be avoided for production installation of xpanse.  ","version":"Next","tagName":"h3"},{"title":"Production Environments​","type":1,"pageTitle":"Database","url":"/xpanse/docs/database#production-environments","content":" We plan to support multiple RDBMS flavors. By support, we mean configurations for the database added and also all functionalities tested.  ","version":"Next","tagName":"h2"},{"title":"MySQL DB​","type":1,"pageTitle":"Database","url":"/xpanse/docs/database#mysql-db","content":" At the moment, this is the only DB that's fully supported. To use MySQL as the database for xpanse, it must be activated by starting the application with profile mysql and by replacing the placeholders for databaseusername and password as below.  java -jar xpanse-runtime-1.0.0-SNAPSHOT.jar \\ -Dspring.profiles.active=mysql \\ -Dspring.datasource.username=${database-username} \\ -Dspring.datasource.password=${database-password}   Versions Supported​  Current supported version of MySQL is 8.1.0. This is the latest LTS release of MySQL.  Default Configuration​  The above command will start the xpanse runtime with the following default configurations:  MySQL running on the same machine where xpanse is running. (Same localhost)Database is listening on port 3306.Name of the database is xpanse. This must be created on the DB server. It won't be automatically created.  The Default configuration file can be found here.  Overriding Default Configuration​  We can override the above two default configurations by starting the application as below by replacing the placeholders with actual values.  java -jar xpanse-runtime-1.0.0-SNAPSHOT.jar \\ -Dspring.profiles.active=mysql \\ -Dspring.datasource.username=&lt;replace-with-db-username&gt; \\ -Dspring.datasource.password=&lt;replace-with-db-password&gt; \\ -D spring.datasource.url=jdbc:mysql://&lt;replace-with-db-hostname&gt;:&lt;replace-with-db-port&gt;/&lt;replace-with-db-name&gt;   secrets better as environment variables It's safe to provide the database-related properties as environment variables rather than passing them directly in the command line. In case of this, the same property name must be set in UPPERCASE and underscore separated instead of dot for all variables.  network between xpanse and its database Using this startup command, the database can run on any machine that's reachable from the xpanse runtime application.  MySQL as a Docker Container​  MySQL offers official Docker images for running a database as a container. More details can be found here on the official page of MySQL website here and on DockerHub here.  Starting new container​  While starting the MySQL docker container for the first time, we can configure database-host, database-port anddatabase-name as below and the same can be used in starting the xpanse runtime using the command described  above.  docker pull mysql:8.1.0   By starting the container with the below command, the database is started by automatically configuring the database with xpanse database name, database user and password. In addition to that, password for the DB root user can also be updated to the value of our choice.  docker run --name ${container-name} \\ -e MYSQL_ROOT_PASSWORD=&lt;replace-with-db-root-password&gt; \\ -e MYSQL_DATABASE=&lt;replace-with-db-name&gt; \\ -e MYSQL_USER=&lt;replace-with-db-user&gt; \\ -e MYSQL_PASSWORD=&lt;replace-with-db-password&gt; \\ -p 3306:&lt;replace-with-db-port&gt; -d mysql:8.1.0   Avoid secrets in command line To avoid passing database related properties in command line, we can use the --env-file option of the docker run command to store all sensitive data.  Database Objects Creation​  The application automatically creates all database objects such as tables needed by the application when it boots up and uses a database for the first time. ","version":"Next","tagName":"h3"},{"title":"Demo","type":0,"sectionRef":"#","url":"/xpanse/docs/FurtherReading/demos","content":"Demo The Eclipse xpanse Demo shows you: how to register, update and delete a service.how to use the service catalog and order services.how to manage the ordered services - monitoring, migrating, etc.how to manage cloud provider credentialsand many more. The demo is available as video stream.","keywords":"","version":"Next"},{"title":"Database Schema Evolution","type":0,"sectionRef":"#","url":"/xpanse/docs/database-schema-evolution","content":"","keywords":"","version":"Next"},{"title":"Use of hdm2ddl library​","type":1,"pageTitle":"Database Schema Evolution","url":"/xpanse/docs/database-schema-evolution#use-of-hdm2ddl-library","content":" Spring Boot uses Hibernate framework's hdm2ddl which does the following  Automatically generate DDL statements based on Entity classes available in the classpath.Execute DDL statements automatically at the server start.  This feature is used in xpanse by default for development purposes and also additionally to prepare a database with all necessary database objects automatically and then use it for generating Liquibase SQL change scripts for the respective release.  hdm2ddl for schema evolution process We use hdm2ddl for schema evolution in order to simplify the process of generating SQL scripts. The developer need not write SQL scripts. Instead, we let hdm2ddl to generate the scripts and prepare the release preparation database and then use Liquibase's feature to auto generate DDL SQL statements based on the database structure.  Manual Process The schema generation process isn't fully automated to ensure the changes are verified by the developer before the release is done. Even though we use Hibernate and Liquibase to generate changes, the developer must still validate the changes generated. This is also the recommended best practice from Liquibase.  ","version":"Next","tagName":"h2"},{"title":"Release preparation​","type":1,"pageTitle":"Database Schema Evolution","url":"/xpanse/docs/database-schema-evolution#release-preparation","content":" When a xpanse release is planned, the developer must prepare the repository with the necessary change scripts using the steps below. The scripts must be pushed and merged to the main branch, and only then the release job must be executed. s  There are always two sets of changes to be prepared for each release of xpanse.  **Full Release Change Log—**This is used when the xpanse release will be installed on a fresh and empty database.**Update change log—**This contains only the difference between the new release and the previous releases.  H2 database scripts included Even though H2 database is used for development purposes, we also deliver SQL scripts for it just for completeness.  ","version":"Next","tagName":"h2"},{"title":"Prepare Full Release Snapshot​","type":1,"pageTitle":"Database Schema Evolution","url":"/xpanse/docs/database-schema-evolution#prepare-full-release-snapshot","content":"   ","version":"Next","tagName":"h3"},{"title":"MySql​","type":1,"pageTitle":"Database Schema Evolution","url":"/xpanse/docs/database-schema-evolution#mysql","content":" Clone the forked xpanse repository. Ensure the fork is fully in sync with the upstream.Checkout a feature branch from main branch.Start a new MySql database container.Start xpanse with mysql profile. Let hbm2ddl create all necessary database objects.Run Liquibase's maven plugin to generate the full release snapshot of the database schema.Rename the generated file name.  # Start MySql Container docker run --name mysql-db -p 3306:3306 -e MYSQL_PASSWORD=Xpanse@2023 -e MYSQL_ROOT_PASSWORD=Xpanse@2023 -e MYSQL_DATABASE=xpanse -e MYSQL_USER=xpanse -d mysql:latest # Start Application with mysql spring profile # Run Liquibase maven plugin cd modules/database mvn liquibase:generateChangeLog -Dliquibase.properties.file=src/main/resources/liquibase/liquibase.mysql.properties # Rename the generated src/main/resources/liquibase/full/mysql/xpanse-REPLACE_NEW_RELEASE-full.mysql.sql file name. # Replace REPLACE_NEW_RELEASE value in the file name with the next planned release value.   ","version":"Next","tagName":"h3"},{"title":"H2​","type":1,"pageTitle":"Database Schema Evolution","url":"/xpanse/docs/database-schema-evolution#h2","content":" Clone the forked xpanse repository. Ensure the fork is fully in sync with the upstream.Checkout a feature branch from main branch.Delete existing H2 DB files from your local file system.Start xpanse with default profiles that enable H2 database. Let hbm2ddl create all necessary database objects.Stop the application. Otherwise, Liquibase can't connect concurrently to the H2 database files while the application is also running.Run Liquibase's maven plugin to generate the full release snapshot of the database schema.Rename the generated file name.  # Start Application with default profiles. # Run Liquibase maven plugin cd modules/database mvn liquibase:generateChangeLog -Dliquibase.properties.file=src/main/resources/liquibase/liquibase.h2.properties # Rename the generated src/main/resources/liquibase/full/h2/xpanse-REPLACE_NEW_RELEASE-full.h2.sql file name. # Replace REPLACE_NEW_RELEASE value in the file name with the next planned release value.   ","version":"Next","tagName":"h3"},{"title":"Prepare upgrade to a release​","type":1,"pageTitle":"Database Schema Evolution","url":"/xpanse/docs/database-schema-evolution#prepare-upgrade-to-a-release","content":" This step is necessary to generate the changes necessary to upgrade a database running on a previous version of the schema to the new version of the schema.    MySql​  Step 1 - Prepare the last released DB Schema. This represents the 'is' state of the schema.  Clone the xpanse upstream repository and checkout GIT tag of the previous released version.Start a new MySql database container with portStart xpanse with mysql profile. Let Hibernate create all necessary database objects.Stop the application.  # Clone the upstream repository and checkout the tag. # Forked repositories will not work. git clone https://github.com/eclipse-xpanse/xpanse.git git checkout $vX.X.X # Start MySql Container cd modules/database/src/main/resources/liquibase/ docker run --name mysql-db-current -p 3306:3306 -e MYSQL_PASSWORD=Xpanse@2023 -e MYSQL_ROOT_PASSWORD=Xpanse@2023 -e MYSQL_DATABASE=xpanse -e MYSQL_USER=xpanse -v ./init.sql:/docker-entrypoint-initdb.d/init.sql:ro -d mysql:latest # Start Application with mysql spring profile   Step 2 - Prepare the Reference DB Schema. This represents the 'to-be' state of the schema.  Clone the forked xpanse repository. Ensure the fork is fully in sync with the upstream.Checkout a feature branch from the main branch.Start a new MySql database container with another name and port.Start xpanse with mysql profile and use the new database. Let Hibernate create all necessary database objects.Stop the application.  # Start MySql Container from the forked repository and database folder cd modules/database/src/main/resources/liquibase/ docker run --name mysql-db-latest -p 3307:3306 -e MYSQL_PASSWORD=Xpanse@2023 -e MYSQL_ROOT_PASSWORD=Xpanse@2023 -e MYSQL_DATABASE=xpanse -e MYSQL_USER=xpanse -v ./init.sql:/docker-entrypoint-initdb.d/init.sql -d mysql:latest # update the application.mysql.properties file - spring.datasource.url=jdbc:mysql://localhost:3307/xpanse spring.activiti.datasource.jdbcUrl=jdbc:mysql://localhost:3307/activiti # Start Application with mysql spring profile   Step 3 - Use Liquibase to compare these two databases and generate the difference change log.  Run the maven plugin command with the goal specified below.Verify the changes generated by Liquibase.Rename the file with correct release numbers.  # Run Liquibase maven plugin cd modules/database mvn liquibase:diff -Dliquibase.properties.file=src/main/resources/liquibase/liquibase.mysql.properties # Rename the generated xpanse-upgrade-REPLACE_OLD_RELEASE-to-REPLACE_NEW_RELEASE.mysql.sql file name. # Replace REPLACE_NEW_RELEASE value in the file name with the next planned release value. # Replace REPLACE_OLD_RELEASE value in the file name with the previous released version.   H2​  Step 1 - Prepare the last released DB Schema. This represents the 'is' state of the schema.  Clone the xpanse upstream repository and checkout GIT tag of the previous released version.Delete all existing H2 database files.Start xpanse with default profiles. Let Hibernate create all necessary database objects.Stop the application.  # checkout git tag spring.datasource.url=jdbc:h2:file:./xpanse-new;DB_CLOSE_ON_EXIT=FALSE   Step 2 - Prepare the Reference DB Schema. This represents the 'to-be' state of the schema.  Use the same cloned xpanse repository from step 1.Checkout a feature branch from main branch.Update spring.datasource.url to use another unique database file.Start xpanse with default profiles. Let Hibernate create all necessary database objects.Stop the application.  # update the application.properties file - spring.datasource.url=jdbc:h2:file:./xpanse-new;DB_CLOSE_ON_EXIT=FALSE   Step 3 - Use Liquibase to compare these two databases and generate the difference change log.  Run the maven plugin.Verify the changes generated by Liquibase.Rename the file with correct release numbers.  # Run Liquibase maven plugin cd modules/database mvn liquibase:diff -Dliquibase.properties.file=src/main/resources/liquibase/liquibase.h2.properties # Rename the generated xpanse-upgrade-REPLACE_OLD_RELEASE-to-REPLACE_NEW_RELEASE.h2.sql file name. # Replace REPLACE_NEW_RELEASE value in the file name with the next planned release value. # Replace REPLACE_OLD_RELEASE value in the file name with the previous released version.   same process for generating upgrade from any release to new release The steps mentioned above aren't only valid for upgrading just previous release to new release. Same steps can be used to upgrade from any of the previous releases to new release. Just the tag to be checked out must be changed based on the requirement.  ","version":"Next","tagName":"h3"},{"title":"Merging Change Log Files​","type":1,"pageTitle":"Database Schema Evolution","url":"/xpanse/docs/database-schema-evolution#merging-change-log-files","content":" Once the change log is generated and verified, a PR must be opened to merge these files into the main branch. Only after this merge is completed, the new release can be done.  ","version":"Next","tagName":"h2"},{"title":"Installing or Upgrading Database​","type":1,"pageTitle":"Database Schema Evolution","url":"/xpanse/docs/database-schema-evolution#installing-or-upgrading-database","content":" The below maven command must be updated with the correct change log file and then can be used to execute the upgrade scripts on the target database.  mvn liquibase:update \\ -Dliquibase.properties.file=src/main/resources/liquibase/liquibase.mysql.properties \\ -Dliquibase.changeLogFile=src/main/resources/liquibase/full/mysql/xpanse-upgrade-v1.0.0-to-v1.0.1.mysql.xml   double check file names Most important parameter is the changeLogFile. This must be doubled checked before executing the command. If we plan to execute multiple change log files, then the current sequence of change log files matter.  ","version":"Next","tagName":"h2"},{"title":"Design Decisions​","type":1,"pageTitle":"Database Schema Evolution","url":"/xpanse/docs/database-schema-evolution#design-decisions","content":" Liquibase is available for many years and has better community support.XML change log used—This is the oldest format in Liquibase and also the most stable one. Found some issues with SQL formatted change logs.  ","version":"Next","tagName":"h2"},{"title":"Known Issues​","type":1,"pageTitle":"Database Schema Evolution","url":"/xpanse/docs/database-schema-evolution#known-issues","content":" If the diff generates a change in data type with the modifyDataType tag, then all constraints on such a column are lost. We must add the constraints back to the change log script. This is a MySQL only limitation. ","version":"Next","tagName":"h2"},{"title":"Resources","type":0,"sectionRef":"#","url":"/xpanse/docs/FurtherReading/resources","content":"Resources Pitch Slide DeckDemo Presentations at events EclipseCon 2022, Open Services Cloud: unleash cloud managed services (video)EclipseCon 2023 - Eclipse xpanse - An Open Services Cloud Project (video)EclipseCon 2023 - Eclipse xpanse - An Open Services Cloud Project (slides)EclipseCon 2023 - Open Cloud Services and an Open Cloud Computing Stack (video)EclipseCon 2023 - Open Cloud Services and an Open Cloud Computing Stack (slides)OpenInfra Day Hungary 2024 Budapest - OCS and SCS - a full stack for deploying from infrastructure up to applications (video)","keywords":"","version":"Next"},{"title":"Developer Setup","type":0,"sectionRef":"#","url":"/xpanse/docs/developer-setup","content":"","keywords":"","version":"Next"},{"title":"Prerequisite tools​","type":1,"pageTitle":"Developer Setup","url":"/xpanse/docs/developer-setup#prerequisite-tools","content":" Java 21 - OpenJDK or Eclipse AdoptiumGIT clientMaven for Java applicationsDockerDocker composeNodeJS for UI moduleNPM for UI module.Go SDK  ","version":"Next","tagName":"h2"},{"title":"Setup and run xpanse project​","type":1,"pageTitle":"Developer Setup","url":"/xpanse/docs/developer-setup#setup-and-run-xpanse-project","content":" To run the main xpanse application, the following steps must be executed.  Fork the xpanse project.Clone the fork.We can then either compile the project using maven and start the application from CLI or just execute the main class XpanseApplication.java from your preferred IDE.  minimum required profiles For developers, the minimum required profiles to be activated are noauth and dev.  ./mvnw clean install -DskipTests java -Dspring.profiles.active=noauth,dev -jar runtime/target/xpanse-runtime-*-SNAPSHOT.jar   To confirm the server is started, you can try to access the swagger UI from your browser.  http://localhost:8080/swagger-ui/index.html   The runtime can be controlled by activating different spring profiles and configuration parameters. More information about this can be found here.  ","version":"Next","tagName":"h2"},{"title":"Dependent Components​","type":1,"pageTitle":"Developer Setup","url":"/xpanse/docs/developer-setup#dependent-components","content":" For regular development activities, xpanse needs some predefined applications from its stack based on the use case. But in most cases, the dependencies are kept as minimal as possible.  Component\tMandatory\tDefaultDatabase\tNo\tH2 Database terra-boot\tNo\tterraform-local tofu-maker\tNo\topentofu-local policy-man\tNo\tNecessary only for testing policies zitadel\tNo\tWe can also use 'noauth' mode UI\tNo\tWe can test APIs directly Redis Cache\tNo\tCaffeine Cache  To set up the above-mentioned components, we can either use a master docker-compose configuration which will start all components with default configuration.  Or we can also make start individual components based on the use case.  ","version":"Next","tagName":"h2"},{"title":"Master Docker Compose​","type":1,"pageTitle":"Developer Setup","url":"/xpanse/docs/developer-setup#master-docker-compose","content":" By executing the below commands, we can start all components in one click.  # Download docker compose configuration file. wget https://raw.githubusercontent.com/eclipse-xpanse/xpanse-relops/refs/heads/main/local/docker-compose.yml # Run the database and application containers. docker compose up --d   local-docker-stack profile to be enabled Then ensure to enable the spring profile local-docker-stack in xpanse application to ensure communication between docker containers and the xpanse running on host.  disable containers if necessary If you think that any of the containers in the docker compose file isn't necessary for your use case, you can simply comment them out.  ","version":"Next","tagName":"h3"},{"title":"Individual Component Configuration​","type":1,"pageTitle":"Developer Setup","url":"/xpanse/docs/developer-setup#individual-component-configuration","content":" ","version":"Next","tagName":"h3"},{"title":"Database​","type":1,"pageTitle":"Developer Setup","url":"/xpanse/docs/developer-setup#database","content":" By default, the development environment uses h2 database. Other than this, xpanse supports MySql as well at the moment.  If you wish to test with MySql, the MySql container can be started using the below command.  docker run --name mysql-db -p 3306:3306 -e MYSQL_PASSWORD=Xpanse@2023 -e MYSQL_ROOT_PASSWORD=xpanse -e MYSQL_DATABASE=xpanse -e MYSQL_USER=xpanse -d mysql:latest   After the container is running, update the value of configuration spring.profiles.active in the xpanse application to append the profile value mysql.  ","version":"Next","tagName":"h3"},{"title":"Redis Cache​","type":1,"pageTitle":"Developer Setup","url":"/xpanse/docs/developer-setup#redis-cache","content":" By default, the development environment uses Caffeine Cache. Other than this, xpanse supports Redis Cache as well at the moment.  If you wish to test with Redis Cache, the Redis container can be started using the below command.  docker run --name redis-cache -p 6379:6379 -e REDIS_PASSWORD=Xpanse@2023 -d redis:latest   After the container is running, update the value of configuration spring.profiles.active in the xpanse application to append the profile value redis.  ","version":"Next","tagName":"h3"},{"title":"Deployer Tools Installation​","type":1,"pageTitle":"Developer Setup","url":"/xpanse/docs/developer-setup#deployer-tools-installation","content":" Depending on the type of deployers you intend to use, the below steps can be used to install the necessary deployers for local development.  Deployer Development These steps are necessary for development activities in the main xpanse application as well as deployer applications such as terra-boot or tofu-maker.  Auto Installation​  During the deployment job, the deployer automatically downloads the necessary deployer binaries. If you have an active and stable connection, then there is nothing to be done in this case.  Using Docker Images​  If you wish to use remote deployers such as terra-boot or tofu-makerthen you can pull the latest docker containers and configure the xpanse runtime to use it.  In this case, the images already contain the default supported Terraform and OpenTofu versions respectively and additionally can install new versions on demand.  docker run -p 9090:9090 -d --name terra-boot --add-host=host.docker.internal:host-gateway ghcr.io/eclipse-xpanse/terra-boot:latest docker run -p 9092:9092 -d --name tofu-maker --add-host=host.docker.internal:host-gateway ghcr.io/eclipse-xpanse/tofu-maker:latest   local-docker-stack profile to be enabled Then ensure to enable the spring profile local-docker-stack in xpanse application to ensure communication between docker containers and the xpanse running on host.  Manual Installation​  If you wish you manually install the deployers for whatever reason, then the below steps are necessary.  Necessary for local or remote deployment This method is necessary for local and remote deployment if the auto installation isn't possible.  Terraform CLI Installation​  Terraform CLI can be installed using the steps from the official guide.  OpenTofu CLI Installation​  Terraform CLI can be installed using the steps from the official guide.  ","version":"Next","tagName":"h3"},{"title":"Auth Layer Installation​","type":1,"pageTitle":"Developer Setup","url":"/xpanse/docs/developer-setup#auth-layer-installation","content":" No Auth Mode​  If feature under development doesn't need any authentication or authorization features, then it can be turned off completely by simply activating the noauth spring profile.  OAuth Mode​  This is the default mode, and by default we use Zitadel as the identity provider. We build a fully configured development Zitadel application and DB images which contains all necessary configurations as well as test users.  Run the below command to start the OAuth stack.  # Download the docker compose configuration. wget https://raw.githubusercontent.com/eclipse-xpanse/xpanse-iam/refs/heads/main/zitadel/local/run/docker-compose.yml # Run the database and application containers. docker compose up --d   Admin User Also, in case you wish to connect and check and change the configuration of the Zitadel instance, the admin user can be used URL—http://localhost:8088 username—zitadel-admin@zitadel.localhost password - Zitadel@123  The necessary end users with multiple combinations of roles and metadata can be found here. Here, we must use the provided email address as the username for OAuth application.  client configuration The zitadel-local spring profile already contains the configuration details necessary to connect to this local Zitadel instance. The developer has no other extra task to be done.  More information about Zitadel can be found here.  ","version":"Next","tagName":"h3"},{"title":"policy-man​","type":1,"pageTitle":"Developer Setup","url":"/xpanse/docs/developer-setup#policy-man","content":" policy-man is a Golang-based application.  Manual start​  We will need Golang SDK to be installed and the steps mentioned herecan be used to start the application.  Docker Image​  Easiest way to simply start the policy-man application using the released docker images.  docker run -p 8090:8090 -d --name policy-man ghcr.io/eclipse-xpanse/policy-man:latest --host=0.0.0.0   No changes necessary on xpanse Irrespective of the above methods used to start policy-man, no changes are necessary on the xpanse side. The default configuration already points to correct local instances of the application.  ","version":"Next","tagName":"h3"},{"title":"UI Module​","type":1,"pageTitle":"Developer Setup","url":"/xpanse/docs/developer-setup#ui-module","content":" UI module requires node and NPM binaries installed on the developer machine.  Auth modes​  Depending on the necessity, the UI can be started with or without auth mode.  # Start with noauth mode npm run start # start with auth mode npm run start-with-zitadel-local   More documentation on UI can be found here. ","version":"Next","tagName":"h3"},{"title":"Network Proxy","type":0,"sectionRef":"#","url":"/xpanse/docs/network-proxy","content":"","keywords":"","version":"Next"},{"title":"Inject Proxy Parameters​","type":1,"pageTitle":"Network Proxy","url":"/xpanse/docs/network-proxy#inject-proxy-parameters","content":" Three parameters - http_proxy, https_proxy and no_proxy control the proxy configuration, and they must be available as environment variables to the runtime.  Variable names can be lowercase or uppercase. If both are available, then the first priority will be given to lower case.  export http_proxy=http://username:password@proxy.example.com:1234 export https_proxy=http://username:password@proxy.example.com:5678 export no_proxy=&quot;localhost&quot;   ","version":"Next","tagName":"h2"},{"title":"Use Proxy Parameters​","type":1,"pageTitle":"Network Proxy","url":"/xpanse/docs/network-proxy#use-proxy-parameters","content":" Since different HTTP clients use different sources and formats for network proxy, the integrations in xpanse can use the following ways to get proxy information.  The environment variables passed to the runtime can be directly used if necessary.xpanse parses the proxy details from environment variables and injects necessary java specific proxy parameters as Java system properties. More information can be found Java network proxies. Libraries which depend on these default Java proxy system properties will work out of the box.Additionally,the complete proxy configuration is exposed as a Spring bean. It can be accessed from anywhere using ProxyConfigurationManager bean. ","version":"Next","tagName":"h2"},{"title":"Observability","type":0,"sectionRef":"#","url":"/xpanse/docs/observability","content":"","keywords":"","version":"Next"},{"title":"OpenTelemetry​","type":1,"pageTitle":"Observability","url":"/xpanse/docs/observability#opentelemetry","content":"   As a default implementation, we use OpenTelemetry to generate and collect telemetry data. The OpenTelemetry provided SDKs and auto instrumentation features helps to generate all necessary data with no additional implementation. Using OpenTelemetry provides an advantage that, we generate data based on standards specified by OpenTelemetry and thereby gives us the flexibility to feed this data to any OpenTelemetry protocol supported backends.  ","version":"Next","tagName":"h3"},{"title":"Telemetry Data Generated​","type":1,"pageTitle":"Observability","url":"/xpanse/docs/observability#telemetry-data-generated","content":" Logs​  All applications involved in the xpanse stack, generates logs with useful information to trace a request end-to-end. Logs have metadata which provides very high level of traceability.  The OpenTelemetry auto instrumentation SDK is then used to export all generated logs from existing logging frameworks to OTEL collector.  Metrics and Traces​  All applications are provided with an option to enable OpenTelemetry's auto instrumentation features which then generates metrics and traces.  ","version":"Next","tagName":"h3"},{"title":"Telemetry Data collector​","type":1,"pageTitle":"Observability","url":"/xpanse/docs/observability#telemetry-data-collector","content":" We can use any product which implements the OpenTelemetry specifications for collecting data. The collector's endpoint must then be configured in the data producing systems for forwarding the generated Telemetry data to the collector.  ","version":"Next","tagName":"h3"},{"title":"Components Generating Observability Data​","type":1,"pageTitle":"Observability","url":"/xpanse/docs/observability#components-generating-observability-data","content":" xpanse - activated using spring profile opentelemetryterra-boot - activated using spring profile opentelemetrypolicy-man - activated by starting OpenTelemetry's instrumentation process together with policy-manzitadel - enabled by default  ","version":"Next","tagName":"h3"},{"title":"Sample Full Stack Observability​","type":1,"pageTitle":"Observability","url":"/xpanse/docs/observability#sample-full-stack-observability","content":" An example on how to enable auto instrumentation for all xpanse components and then export data to OTEL collector which can be later feed into Grafana monitoring stack can be found here. ","version":"Next","tagName":"h3"},{"title":"Introduction","type":0,"sectionRef":"#","url":"/xpanse/docs/introduction","content":"","keywords":"","version":"Next"},{"title":"Goals of the project​","type":1,"pageTitle":"Introduction","url":"/xpanse/docs/introduction#goals-of-the-project","content":" xpanse is a project aimed reshaping the cloud services ecosystem:  Everybody should be able to sell native managed services.End users will have just one channel to buy services.Standardize service definition process by creating a cloud-agnostic service configuration language.Make service portability a default feature.  ","version":"Next","tagName":"h2"},{"title":"Getting Started​","type":1,"pageTitle":"Introduction","url":"/xpanse/docs/introduction#getting-started","content":" To understand the project and its implementation better, refer to the architecture, modes and service configuration language pages.  ","version":"Next","tagName":"h2"},{"title":"Current Features​","type":1,"pageTitle":"Introduction","url":"/xpanse/docs/introduction#current-features","content":" The current version of xpanse supports the following features   ","version":"Next","tagName":"h2"},{"title":"Package Structure","type":0,"sectionRef":"#","url":"/xpanse/docs/package-structure","content":"","keywords":"","version":"Next"},{"title":"modules​","type":1,"pageTitle":"Package Structure","url":"/xpanse/docs/package-structure#modules","content":" This maven module is a parent module for most of the other maven modules within xpanse.  api​  Contains all the REST API controller classes and the exception handlers.  async​  Contains all classes related to asynchronous processing configuration for different use cases within xpanse.  common​  Contains generic and utility classes which are used by multiple different modules.  credential​  contains all logic and caching for cloud provider credentials.  database​  contains all entities, DB queries and JPA related code.  deployment​  contains all code related to service deployment. All service deployers such Terraform, OpenTofu, etc are also implemented in this module.  logging​  contains all code and configuration related to logging.  models​  contains all data models and exceptions that are exposed externally in our APIs. All data models here are automatically exported in the OpenAPI doc.  Avoid cyclic dependencies with models package Ensure that this module doesn't have dependency to any other xpanse modules else it will result in cycle dependencies errors. This package must not contain any internal data classes.  monitor​  contains all code related to fetching and caching service metrics for deployed services.  observability​  contains all code related to observability and traceability of the xpanse stack.  monitoring vs observability packages monitor module is for monitoring the customer's deployed clouds services whereas observability module is for monitoring the xpanse stack itself.  orchestrator​  This package is the central package for orchestrating service deployment, monitoring, cloud provider credentials, service state management. It also has all the internal data models used by xpanse for all use-cases.  Policy​  This package contains all code related to OPA policy validation. Also, the policy-man REST client is part of this package.  security​  This package contains all code and configuration related to security within xpanse. It contains the Oauth2 security of xpanse APIs and also the encryption of sensitive order data.  servicetemplate​  contains all code for managing service templates.  workflow​  contains all code and configuration related to managing Activiti workflow engine.  ","version":"Next","tagName":"h2"},{"title":"plugins​","type":1,"pageTitle":"Package Structure","url":"/xpanse/docs/package-structure#plugins","content":" contains all cloud provider specific integrations. Under plugins maven module, there is one submodule for each cloud provider supported by xpanse.  ","version":"Next","tagName":"h2"},{"title":"runtime​","type":1,"pageTitle":"Package Structure","url":"/xpanse/docs/package-structure#runtime","content":" This is the package which contains the main spring boot class and also pulls all other maven modules listed above as dependencies to generate the final jar. ","version":"Next","tagName":"h2"},{"title":"Modes","type":0,"sectionRef":"#","url":"/xpanse/docs/modes","content":"","keywords":"","version":"Next"},{"title":"Cloud Provider Integrated Mode​","type":1,"pageTitle":"Modes","url":"/xpanse/docs/modes#cloud-provider-integrated-mode","content":" In this mode, xpanse is deployed on the management zone of the specific cloud provider that supports xpanse.  With this mode, the services from software vendors are offered as fully native managed services on that specific cloud provider. The end user won't notice if the service is from an independent software vendor or from the cloud provider itself.    ","version":"Next","tagName":"h3"},{"title":"App Store Mode​","type":1,"pageTitle":"Modes","url":"/xpanse/docs/modes#app-store-mode","content":" In this mode, xpanse is deployed outside any specific cloud provider's management layer. Using this mode, the service vendors can use the xpanse platform to sell services on any cloud providers supported by xpanse.  The end user then uses this one stop-shop to consume services from the cloud provider of his choice.    ","version":"Next","tagName":"h3"},{"title":"High-Level Sequence Diagram​","type":1,"pageTitle":"Modes","url":"/xpanse/docs/modes#high-level-sequence-diagram","content":" A very high level happy-case service offering and service ordering via xpanse will look like below.   ","version":"Next","tagName":"h2"},{"title":"Running in Production","type":0,"sectionRef":"#","url":"/xpanse/docs/production","content":"","keywords":"","version":"Next"},{"title":"Run using jar​","type":1,"pageTitle":"Running in Production","url":"/xpanse/docs/production#run-using-jar","content":" Download the released runtime jar from GitHub releases. You can get the latest from here.  After downloading, we can start the jar file using the Java command.  java -jar xpanse-runtime-*.jar   ","version":"Next","tagName":"h2"},{"title":"Run using Docker image​","type":1,"pageTitle":"Running in Production","url":"/xpanse/docs/production#run-using-docker-image","content":" You can start the runtime using our released docker image, and this is the preferred way. This image contains all necessary tools preinstalled. Below is an example of running a single instance of xpanse.  $ docker pull ghcr.io/eclipse-xpanse/xpanse:${release-version} $ docker run -d -p 8080:8080 --name xpanse -e &quot;SPRING_PROFILES_ACTIVE=oauth,zitadel,mysql&quot; ghcr.io/eclipse-xpanse/xpanse:${release-version} $ docker logs xpanse   Avoid properties in command line It's safe to provide all properties as environment variables rather than passing them directly in the command line. In case of this, the same property name must be set in UPPERCASE for all four variables. For running, using docker image, we can use the --env-file option of the docker run command to store all sensitive data. Again, here the property names must be in UPPERCASE.  Running API behind a proxy​  For running the runtime application behind a proxy, we must ensure that the proxy forwards the correct X-Forwarded-*headers to the API. This is necessary since the API has some features where the links to HTML pages are returned and this link will have the correct protocol and host only when these headers are set.  In the case of NGINX, the configuration will look like this  proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Proto https; proxy_set_header Host $host;  ","version":"Next","tagName":"h2"},{"title":"Request Audit","type":0,"sectionRef":"#","url":"/xpanse/docs/request-audit","content":"","keywords":"","version":"Next"},{"title":"Extracting CSP Data​","type":1,"pageTitle":"Request Audit","url":"/xpanse/docs/request-audit#extracting-csp-data","content":" If xpanse is running with just one CSP plugin activated, then all services are by default audited by the logic implemented by that specific CSP plugin.  if more than one CSP is activated in the xpanse runtime, then we extract the CSP name from the request URI or the request body and then forward the audit request to the corresponding CSP plugin.  ","version":"Next","tagName":"h2"},{"title":"Configure Audit for API Methods​","type":1,"pageTitle":"Request Audit","url":"/xpanse/docs/request-audit#configure-audit-for-api-methods","content":" Developers who write API (controller) methods must annotate these API methods with the AuditApiRequest annotation. The annotation must also be provided with the information on how the CSP value can be extracted from the method's input data.  All logic to extract CSP information from request data can be found here. This must be extended if there are any new request data models. ","version":"Next","tagName":"h3"},{"title":"Plugins","type":0,"sectionRef":"#","url":"/xpanse/docs/plugins","content":"","keywords":"","version":"Next"},{"title":"Plugin Implementation​","type":1,"pageTitle":"Plugins","url":"/xpanse/docs/plugins#plugin-implementation","content":" A plugin is a class that implements the OrchestratorPlugininterface. The plugin implementation class must also be a Spring bean.  With the plugin implementation, the plugin will provide the cloud-specific implementation of handling credentials, reading monitoring data, handling infrastructure resources data provisioned via deployers, etc.  ","version":"Next","tagName":"h2"},{"title":"Plugin Activation​","type":1,"pageTitle":"Plugins","url":"/xpanse/docs/plugins#plugin-activation","content":" A plugin for a cloud provider is activated by default. But a plugin might need one or more mandatory configurations available for it to work. These mandatory configuration properties must be declared when the plugins are implemented. If any of the required configuration properties aren't available, then such plugin is simply not considered for processing any requests.  Variables from environment These mandatory variables will also be injected into the deployment environment as environment variables.  ","version":"Next","tagName":"h2"},{"title":"Openstack​","type":1,"pageTitle":"Plugins","url":"/xpanse/docs/plugins#openstack","content":" Mandatory configuration properties are the following  OS_AUTH_URL - Keystone URL of the Openstack installation.  Other optional configuration properties  OS_SERVICE_PROJECT - Openstack project to be used to get monitoring information. All metrics data is stored in a different central project. If this isn't provided, then the project where the resource is hosted is used to get the metrics data.OS_PROXY_HOST and OS_PROXY_PORT - Proxy server information to reach the Openstack installation.OS_SSL_DISABLED - If set to true, the certificate validation on the REST API calls to Openstack installation will be disabled.  ","version":"Next","tagName":"h3"},{"title":"HUAWEI CLOUD​","type":1,"pageTitle":"Plugins","url":"/xpanse/docs/plugins#huawei-cloud","content":" No mandatory configuration properties required.  ","version":"Next","tagName":"h3"},{"title":"FlexibleEngine​","type":1,"pageTitle":"Plugins","url":"/xpanse/docs/plugins#flexibleengine","content":" No mandatory configuration properties required.  ","version":"Next","tagName":"h3"},{"title":"SCS - Sovereign Cloud Stack​","type":1,"pageTitle":"Plugins","url":"/xpanse/docs/plugins#scs---sovereign-cloud-stack","content":" No mandatory configuration properties required. But OS_AUTH_URL which is the keystone base URL of the SCS installation must be passed one of the fixed deployment variables as below in all the service registration requests.  name: OS_AUTH_URL description: SCS cloud instance to be used. kind: fix_env # this is important here. dataType: string mandatory: true sensitiveScope: none value: 'https://example.com'  ","version":"Next","tagName":"h3"},{"title":"Policies","type":0,"sectionRef":"#","url":"/xpanse/docs/policies","content":"","keywords":"","version":"Next"},{"title":"OPA - Open Policy Agent​","type":1,"pageTitle":"Policies","url":"/xpanse/docs/policies#opa---open-policy-agent","content":"   Policies in OPA must be written in Rego which is a Policy Language designed by OPA project. More details on how policies can be written using Rego can be found here.  ","version":"Next","tagName":"h2"},{"title":"Example Use Cases​","type":1,"pageTitle":"Policies","url":"/xpanse/docs/policies#example-use-cases","content":" As an End user, we can use policies to validate if the managed service creates any resources or configuration in the end user's cloud infrastructure which may be not allowed by his/her organization.As a cloud service provider, we can use policies to validate if the service is being correctly deployed.As an integration test to validate if the service is deploying exactly what it's supposed to do.and many more.  ","version":"Next","tagName":"h2"},{"title":"End User Policies​","type":1,"pageTitle":"Policies","url":"/xpanse/docs/policies#end-user-policies","content":" End users can upload their OPA policies and these policies will be validated for all services ordered by the end user.  ","version":"Next","tagName":"h2"},{"title":"CSP Policies​","type":1,"pageTitle":"Policies","url":"/xpanse/docs/policies#csp-policies","content":" Cloud service providers can define policies for each of their service. These policies will be validated whenever the service is deployed.  ","version":"Next","tagName":"h2"},{"title":"Policy Management APIs​","type":1,"pageTitle":"Policies","url":"/xpanse/docs/policies#policy-management-apis","content":" xpanse provides two sets of APIs for managing life-cycle of policies.  Services Policies Management - To add, update and delete service level policies by CSP.User Policies Management - To add, update and delete user level policies by end user.  ","version":"Next","tagName":"h2"},{"title":"Policy-Man​","type":1,"pageTitle":"Policies","url":"/xpanse/docs/policies#policy-man","content":" xpanse uses an application called policy-man for the below purposes  Rego syntax validation when policies are uploaded.OPA policy validation when services are deployed. ","version":"Next","tagName":"h2"},{"title":"Cloud Service Provider","type":0,"sectionRef":"#","url":"/xpanse/docs/Roles/csp","content":"","keywords":"","version":"Next"},{"title":"Cloud provider integrated mode​","type":1,"pageTitle":"Cloud Service Provider","url":"/xpanse/docs/Roles/csp#cloud-provider-integrated-mode","content":" In this mode, the role of CSP will be to install xpanse stack on their management layer and make it available for ISVs to register their services. Once registered, CSPs must validate and approve services that can be allowed to be hosted in their managed services catalog.  ","version":"Next","tagName":"h2"},{"title":"App store mode​","type":1,"pageTitle":"Cloud Service Provider","url":"/xpanse/docs/Roles/csp#app-store-mode","content":" In this mode, the role of CSP is to validate and approve services the ISV can sell on their infrastructure.  auto approve service templates If a CSP wish to simply auto approve service templates, it can be configured in the plugin accordingly by implementing this method. ","version":"Next","tagName":"h2"},{"title":"End User","type":0,"sectionRef":"#","url":"/xpanse/docs/Roles/end-user","content":"","keywords":"","version":"Next"},{"title":"Cloud provider integrated mode​","type":1,"pageTitle":"End User","url":"/xpanse/docs/Roles/end-user#cloud-provider-integrated-mode","content":" When xpanse is deployed in the cloud provider integrated mode, then the user has no impact. xpanse becomes completely transparent to end user.  ","version":"Next","tagName":"h2"},{"title":"Access​","type":1,"pageTitle":"End User","url":"/xpanse/docs/Roles/end-user#access","content":" End users use the cloud provider's console and API using his/her IAM credentials of the specific cloud provider to consume xpanse based services.  ","version":"Next","tagName":"h3"},{"title":"App store mode​","type":1,"pageTitle":"End User","url":"/xpanse/docs/Roles/end-user#app-store-mode","content":" When xpanse is deployed in the app store mode, then the end user can perform the following actions via xpanse APIs.  Get access to the app store.Add cloud credentials to the key store.Browse through services catalogChoose the service they wish to order.Provide all necessary parameters to deploy service.Deploy service.  After the service is deployed, the end user can use and manage the full lifecycle of the service.  ","version":"Next","tagName":"h2"},{"title":"Access​","type":1,"pageTitle":"End User","url":"/xpanse/docs/Roles/end-user#access-1","content":" Accessing the app store depends on the auth provider used by the app store. End user needs an account based on the app store setup.  Additionally, if end user wishes to have the service hosted on their own cloud tenants, then the cloud provider specific tenant credentials must be added in the app store. This also depends on the kind of credentials supported by the respective cloud provider in it's xpanse plugin. ","version":"Next","tagName":"h3"},{"title":"Service Changes","type":0,"sectionRef":"#","url":"/xpanse/docs/service-changes","content":"Service Changes A successfully deployed service can be modified at different levels. Below the options supported by xpanse. Modify Parameters - This option will allow user to change the service parameters that were used to deploy the service. This may or may not recreate the service based on how the deployment scripts are implemented and APIs supported by the CSP.Upscale/Downscale - This option will allow to change the flavor of the deployed service. This again may or may not recreate the service depending on how the deployment scripts are implemented and APIs supported by the CSP. For example, if a new flavor needs a different VM size, some CSPs allow updating the existing VM size but some don't. Depending on such capabilities, the flavor change can be with or without service/data loss.Destroy - This option can be used to delete the service. It will delete all resources used by a service.Start/Stop/Restart - This option can be used to start/stop/restart the service. Currently, this option controls only virtual machines that are used in the service.Migrate - This option is used to migrate an existing service to another CSP.Lock/Unlock - This option can be used for locking the services from accidental deletion or modifications.Update service configuration parameters.","keywords":"","version":"Next"},{"title":"Independent Service Vendor","type":0,"sectionRef":"#","url":"/xpanse/docs/Roles/isv","content":"","keywords":"","version":"Next"},{"title":"Cloud provider integrated mode​","type":1,"pageTitle":"Independent Service Vendor","url":"/xpanse/docs/Roles/isv#cloud-provider-integrated-mode","content":" In case of cloud provider integrated mode, ISVs will use the cloud provider specific console and API end points to access the xpanse APIs to register their services.  ","version":"Next","tagName":"h2"},{"title":"Access​","type":1,"pageTitle":"Independent Service Vendor","url":"/xpanse/docs/Roles/isv#access","content":" ISVs use the cloud provider's console and API using his/her IAM credentials to register their native managed services using xpanse.  ","version":"Next","tagName":"h3"},{"title":"App store mode​","type":1,"pageTitle":"Independent Service Vendor","url":"/xpanse/docs/Roles/isv#app-store-mode","content":" In case of app store mode, the xpanse uses the APIs hosted on the app store's management area to sell services on any cloud provider they wish and is supported by the app store  ","version":"Next","tagName":"h2"},{"title":"Access​","type":1,"pageTitle":"Independent Service Vendor","url":"/xpanse/docs/Roles/isv#access-1","content":" Accessing the app store depends on the auth provider used by the app store. ISV needs an account based on the app store setup and must have the isv role associated with it.  Additionally, if ISV wishes sell service-vendor hosted service, then the specific tenant credentials must be configured in the app store. This depends on the kind of credentials supported by the respective cloud provider in it's xpanse plugin. ","version":"Next","tagName":"h3"},{"title":"Sensitive Data","type":0,"sectionRef":"#","url":"/xpanse/docs/senstive-data","content":"","keywords":"","version":"Next"},{"title":"Types of sensitive data handled​","type":1,"pageTitle":"Sensitive Data","url":"/xpanse/docs/senstive-data#types-of-sensitive-data-handled","content":" Sensitive data in the cloud provider credentials data.Sensitive data in the service deployment requests.Sensitive data in the service access parameters.  ","version":"Next","tagName":"h2"},{"title":"Algorithm for encryption​","type":1,"pageTitle":"Sensitive Data","url":"/xpanse/docs/senstive-data#algorithm-for-encryption","content":" We can use any algorithm supported by Java as mentioned here. By default, the supported algorithm is AES with CBC mode. The default configuration can be overridden by updating the below three parameters.  xpanse.secrets.encryption.algorithm.name=AES xpanse.secrets.encryption.algorithm.mode=CBC xpanse.secrets.encryption.algorithm.padding=ISO10126Padding   ","version":"Next","tagName":"h2"},{"title":"Secret Key​","type":1,"pageTitle":"Sensitive Data","url":"/xpanse/docs/senstive-data#secret-key","content":" This is the most critical piece of the solution since this key is used to encrypt all secrets stored and hence can't be compromised.  The below two properties must be used to provide the values of the secret key and initial vector  xpanse.secrets.encryption.secrete.key.value=&quot;&quot; # If the selected algorithm supports/needs initial vector, then provide this value as well. xpanse.secrets.encryption.initial.vector=&quot;&quot;   generating random secure keys There are many ways do it and one of the easy and straightforward ways is to use the online tool - https://acte.ltd/utils/randomkeygen. Here the 'Encryption key 256', must be used as the secret key and 'Basic 16' as the initial vector.  ","version":"Next","tagName":"h2"},{"title":"Secret Key in file​","type":1,"pageTitle":"Sensitive Data","url":"/xpanse/docs/senstive-data#secret-key-in-file","content":" The secret key can be provided either as a file or directly injected as a property. If a secret key file is provided, then the key in the file gets the priority. Application checks if the file is available in the provided path and if the file isn't empty. If both aren't true, then it tries to get the secret key from the configuration parameter directly.  Location of the secret key can be provided by the below configuration parameter. This must be a fully qualified path that's accessible to xpanse runtime.  xpanse.secrets.encryption.secrete.key.file=&quot;&quot;   a valid secret key is mandatory If a valid secret key isn't found from both sources, then the application startup will fail. ","version":"Next","tagName":"h2"},{"title":"Service Hosting Types","type":0,"sectionRef":"#","url":"/xpanse/docs/service-hosting","content":"","keywords":"","version":"Next"},{"title":"Available Service Hosting Types​","type":1,"pageTitle":"Service Hosting Types","url":"/xpanse/docs/service-hosting#available-service-hosting-types","content":" ","version":"Next","tagName":"h2"},{"title":"Self-Hosted​","type":1,"pageTitle":"Service Hosting Types","url":"/xpanse/docs/service-hosting#self-hosted","content":" In this case, the service is deployed on the end customer's cloud account. When a service is self-hosted, the end user can view more information such as service deployment errors, etc. The service vendor only can view only very basic details of the deployed service. The service vendor must contact the end user in case any information is required.  ","version":"Next","tagName":"h3"},{"title":"Service-Vendor-Hosted​","type":1,"pageTitle":"Service Hosting Types","url":"/xpanse/docs/service-hosting#service-vendor-hosted","content":" In this case, the service is deployed on the service vendor's cloud account. When a service is service-vendor hosted, the end user has very restricted access to the deployed service details. The user will only receive the end point details to access the service.  The service vendor has more access to the service deployment details. But the service vendor can't access the end point details.  This mode makes more sense to host a SaaS type of managed service.  ","version":"Next","tagName":"h3"},{"title":"Credentials​","type":1,"pageTitle":"Service Hosting Types","url":"/xpanse/docs/service-hosting#credentials","content":" Service vendors can use the ISV Cloud Credentials Management services to manage the cloud accounts for service deployments.  End users can use the User Cloud Credentials Management services to manage the cloud accounts for service deployments.  ","version":"Next","tagName":"h2"},{"title":"Service With Multiple Service Hosting Types​","type":1,"pageTitle":"Service Hosting Types","url":"/xpanse/docs/service-hosting#service-with-multiple-service-hosting-types","content":" The service vendor must specify the service hosting type in the service template. One service template can only offer a service on only one hosting type.  This is means, when the service vendor want to offer service on multiple hosting types, then one service template must be registered for each hosting type. ","version":"Next","tagName":"h2"},{"title":"Service Configuration","type":0,"sectionRef":"#","url":"/xpanse/docs/service-configuration","content":"","keywords":"","version":"Next"},{"title":"Service Deployment","type":0,"sectionRef":"#","url":"/xpanse/docs/service-deployment","content":"","keywords":"","version":"Next"},{"title":"Deployer Implementation​","type":1,"pageTitle":"Service Deployment","url":"/xpanse/docs/service-deployment#deployer-implementation","content":" Every deployer must implement the methods defined in the Deployer interface. The deployer implementation will abstract all the service deployment tasks that must be executed for provisioning the service and return the result as DeployResult object.  ","version":"Next","tagName":"h2"},{"title":"Processing Deployment Results​","type":1,"pageTitle":"Service Deployment","url":"/xpanse/docs/service-deployment#processing-deployment-results","content":" Each deployer can put all the necessary information about deployment results to a privateProperties map in DeployResult object.  Resource Handlers​  Each plugin must implement a DeployResourceHandler for each DeployerKind and this will be invoked to extract the cloud resources deployed from the DeployerResult.    ","version":"Next","tagName":"h3"},{"title":"Asynchronous Processing​","type":1,"pageTitle":"Service Deployment","url":"/xpanse/docs/service-deployment#asynchronous-processing","content":" Since a service deployment can take longer depending on the complexity of the service and the resources required for it, the entire deployment process is implemented to run asynchronously.  After the deploy or the destroy request is submitted, the runtime validates the request and the client receives an accepted/rejected message synchronously. After this, the runtime hands over the deployment request to another thread which executes the deployment independently.  The clients can then fetch the status of the deployment using getDeployedServiceDetailsById service.  ","version":"Next","tagName":"h2"},{"title":"Terraform​","type":1,"pageTitle":"Service Deployment","url":"/xpanse/docs/service-deployment#terraform","content":" Terraform script that must be executed must be passed as is, in the service definition when the service is registered. You can find examples of how the script can look like in the samples folder.  Terraform binaries required Terraform binaries must be installed on the system where the runtime is running. Or use our docker image, which contains all required software pre-installed.  ","version":"Next","tagName":"h2"},{"title":"Terraform Boot​","type":1,"pageTitle":"Service Deployment","url":"/xpanse/docs/service-deployment#terraform-boot","content":" To offload the service deployment tasks from the xpanse's main runtime engine, we created a project called terra-boot. We used this application for all our Terraform related tasks.  ","version":"Next","tagName":"h3"},{"title":"OpenTofu​","type":1,"pageTitle":"Service Deployment","url":"/xpanse/docs/service-deployment#opentofu","content":" OpenTofu script that must be executed must be passed as is, in the service definition when the service is registered. You can find examples of how the script can look like in the samples folder.  OpenTofu binaries required OpenTofu binaries must be installed on the system where the runtime is running. Or use our docker image, which contains all required software pre-installed.  ","version":"Next","tagName":"h2"},{"title":"Tofu Maker​","type":1,"pageTitle":"Service Deployment","url":"/xpanse/docs/service-deployment#tofu-maker","content":" To offload the service deployment tasks from the xpanse's main runtime engine, we created a project called tofu-maker. We used this application for all our OpenTofu related tasks.  ","version":"Next","tagName":"h3"},{"title":"Script Validation​","type":1,"pageTitle":"Service Deployment","url":"/xpanse/docs/service-deployment#script-validation","content":" As part of the service registration process, xpanse validates the syntax of the Terraform script provided. The service is registered only when the script validation is successful.  We use the Terraform validate feature to achieve this.  ","version":"Next","tagName":"h3"},{"title":"Script Execution​","type":1,"pageTitle":"Service Deployment","url":"/xpanse/docs/service-deployment#script-execution","content":" When the customer orders the service, the following happens  Generate a unique ID for the service requested. This is based on UUID.Create a folder with this ID as the name.From the runtime, call the start the Terraform process outside the JVM to do the following  Inside the folder, the runtime will then create a Terraform workspace.Execute the Terraform scripts  Storing deployment states in database To avoid dependency on the filesystem, the state terraform.tfstate file contents are copied to the database at the end of the service deployment. We reuse this later when the customer requests to destroy the service.  ","version":"Next","tagName":"h3"},{"title":"Reading Script Output​","type":1,"pageTitle":"Service Deployment","url":"/xpanse/docs/service-deployment#reading-script-output","content":" After the runtime starts the Terraform script as a system command, it then starts to listen to stdout and stderr of the job to gather all output generated from the script.  ","version":"Next","tagName":"h3"},{"title":"Capturing Service Details​","type":1,"pageTitle":"Service Deployment","url":"/xpanse/docs/service-deployment#capturing-service-details","content":" For any service ordered by the customer, it's important to return the details of the ordered service. Such as the IP addresses, credentials, etc. which the customer must use to start consuming the service.  We use the Terraform output feature to get such information from the service deployment. So it's important to add the output section to the scripts. The same is also used by the xpanse UI for displaying the service details. ","version":"Next","tagName":"h3"},{"title":"Service Configuration Details in Service Template​","type":1,"pageTitle":"Service Configuration","url":"/xpanse/docs/service-configuration#service-configuration-details-in-service-template","content":" Services that would like to allow end users to change service configuration after the service is deployed must provide the following information in the service template.    ","version":"Next","tagName":"h2"},{"title":"Configuration Managers​","type":1,"pageTitle":"Service Configuration","url":"/xpanse/docs/service-configuration#configuration-managers","content":" Configuration manager is nothing but a compute component in the service deployment.  Example of Configuration Manager In case of a service like Kafka, it consists of multiple components such as zookeeper and broker, So the configuration manager in this case can either be zookeeper or broker.  For every configuration manager, the ISV must declare the location of the scripts, the script name, dependencies, etc in the service template.  ","version":"Next","tagName":"h3"},{"title":"Configuration Parameters​","type":1,"pageTitle":"Service Configuration","url":"/xpanse/docs/service-configuration#configuration-parameters","content":" Configurable service parameters must be listed in the service template. For each configuration parameter, details such as name, description, default, value, configuration manager name (see above) must be provided.  Relation between configuration parameter and manager A service template can have 0:N configuration parameters.For each configuration parameter, there must be 1:1 configuration manager.For each service that multiple parameters, there must be 1:N configuration managers.  ","version":"Next","tagName":"h3"},{"title":"Single Configuration Manager​","type":1,"pageTitle":"Service Configuration","url":"/xpanse/docs/service-configuration#single-configuration-manager","content":" For the below use cases, there would be just one single configuration manager. All configuration parameters in the service template will refer to this single configuration manager.  When the service contains only one component. Example a simple database that runs inside a single VM.When the service contains multiple components but the configuration update script is implemented in such a way that it just executes on component, and it automatically connects to other components to update changes.When the service contains multiple components but only central component such as etcd manages configuration for all components.  ","version":"Next","tagName":"h3"},{"title":"Configuration Manager in Deployment Script​","type":1,"pageTitle":"Service Configuration","url":"/xpanse/docs/service-configuration#configuration-manager-in-deployment-script","content":" Since a configuration manager is just a component in the service deployment itself, we must ensure that service deployment script creates a component with this name. Otherwise, the service template validation will fail.  Configuration manager in HCL When we define the deployment script using HCL which is used by Terraform and OpenTofu deployers, the name of the configuration manager must also be the label assigned to the compute resource of that service component. In the below example, &quot;zookeeper&quot; can be used as a valid configuration manager in the service template resource &quot;openstack_compute_instance_v2&quot; &quot;zookeeper&quot; { availability_zone = local.availability_zone name = &quot;ecs-tf-${random_id.new.hex}&quot; flavor_name = var.flavor_name security_groups = [ local.secgroup_name ] image_id = data.openstack_images_image_v2.image.id admin_pass = local.admin_passwd network { uuid = local.vpc_id } }   ","version":"Next","tagName":"h2"},{"title":"Agent Installation​","type":1,"pageTitle":"Service Configuration","url":"/xpanse/docs/service-configuration#agent-installation","content":" Every component which is also acts as a configuration manager, must install the xpanse agent as part of its resource creation.  Read more about xpanse agents here.  ","version":"Next","tagName":"h3"},{"title":"runOnlyOnce Parameter​","type":1,"pageTitle":"Service Configuration","url":"/xpanse/docs/service-configuration#runonlyonce-parameter","content":" runOnlyOnce parameter is used to define when multiple instance of a component of the service exists but the configuration update should be executed on only one of them.  This means the configuration change will be  either be automatically synced to other instances of the component orthe configuration update job on one instance connects to all other instances and makes the necessary configuration changes.  ","version":"Next","tagName":"h2"},{"title":"Default Configuration​","type":1,"pageTitle":"Service Configuration","url":"/xpanse/docs/service-configuration#default-configuration","content":" When the service is first created, the default value of each of the configuration parameter is stored as the current configuration of the service in the xpanse database. Any further changes to the configuration of the service will be based on this default configuration.  ","version":"Next","tagName":"h2"},{"title":"Supported Configuration Update Tools​","type":1,"pageTitle":"Service Configuration","url":"/xpanse/docs/service-configuration#supported-configuration-update-tools","content":" Currently, Ansible is supported as a configuration update tool. This means, ISVs must implement one Ansible playbook for each configuration manager.  ","version":"Next","tagName":"h2"},{"title":"Ansible Code Location​","type":1,"pageTitle":"Service Configuration","url":"/xpanse/docs/service-configuration#ansible-code-location","content":" All Ansible code must be stored in a GIT repository and the repository must be reachable from the  Future plans The plan is to allow to download the scripts from locations other than GIT repositories in future releases.  ","version":"Next","tagName":"h3"},{"title":"Install Python​","type":1,"pageTitle":"Service Configuration","url":"/xpanse/docs/service-configuration#install-python","content":" Agent which executes the Ansible playbooks expects the required Python version to be already installed.  Use default Python version As a workaround, we can simply use the default Python version that comes with the OS installation and then have the Python installation task itself as a preparation task in the configuration update playbook and then use the new version of Python in the actual configuration update tasks. But we must then ensure that the Python can be downloaded from a source which is reachable from the compute node.  ","version":"Next","tagName":"h3"},{"title":"Install Ansible​","type":1,"pageTitle":"Service Configuration","url":"/xpanse/docs/service-configuration#install-ansible","content":" Ansible can be installed in the configuration manager node as part of the service deployment or let the agent install it as part of the first configuration update request via the dependencies install step documented below.  ","version":"Next","tagName":"h3"},{"title":"Install Dependencies​","type":1,"pageTitle":"Service Configuration","url":"/xpanse/docs/service-configuration#install-dependencies","content":" All Python dependencies can be also be installed using the required Python version. Agent uses pip install to install Python dependencies from PyPi repository.  Install Dependencies from other sources Instead of using dependencies from PyPi repository, we can also simply put the dependencies in any other accessible remote directory and let the Ansible playbook to first download and install them.  ","version":"Next","tagName":"h3"},{"title":"Service Inventory​","type":1,"pageTitle":"Service Configuration","url":"/xpanse/docs/service-configuration#service-inventory","content":" When xpanse returns a configuration change request to the agent, the request also contains the full inventory of the service in the Ansible Inventory JSON format. It can be directly used by the playbooks.  Working with localhost In some cases, the inventory isn't needed at all since the Ansible job runs directly on the host. So simply delegating the playbook to localhost will suffice in such cases.  ","version":"Next","tagName":"h3"},{"title":"Extra vars​","type":1,"pageTitle":"Service Configuration","url":"/xpanse/docs/service-configuration#extra-vars","content":" The requested configuration of the service is returned to the agent which then passes this information to the Ansible playbook execution as extra_vars.  The requested configuration always contains the combined final configuration of the service. This means, the changed parameters with updated values and the unchanged parameters with the existing values.  Avoid points of failure Even though xpanse supports setting up of complete Ansible environment with the agent, it's always better to prepare all necessary setup required in the configuration manager nodes as part of the service deployment itself to avoid any unexpected failures. For example, Network Error. We can even consider preparing the complete Ansible environment in the VM image that's used by the service. ","version":"Next","tagName":"h3"},{"title":"Service Template Reviews","type":0,"sectionRef":"#","url":"/xpanse/docs/service-template-reviews","content":"","keywords":"","version":"Next"},{"title":"Approve/Reject Service Template​","type":1,"pageTitle":"Service Template Reviews","url":"/xpanse/docs/service-template-reviews#approvereject-service-template","content":" CSP can then review the service template based on their internal processes and then either approve or reject the service registration request.  ","version":"Next","tagName":"h2"},{"title":"CSP Role​","type":1,"pageTitle":"Service Template Reviews","url":"/xpanse/docs/service-template-reviews#csp-role","content":" If the xpanse runtime is OAuth protected and RBAC enabled, then the user must have csp role to be able to review the service templates which are submitted.  ","version":"Next","tagName":"h3"},{"title":"CSP Name in Metadata​","type":1,"pageTitle":"Service Template Reviews","url":"/xpanse/docs/service-template-reviews#csp-name-in-metadata","content":" CSPs must also have metadata in the identity provider configured to allow user to be linked to a specific CSP. Details for this can found here. ","version":"Next","tagName":"h3"},{"title":"Service Portability","type":0,"sectionRef":"#","url":"/xpanse/docs/service-portability","content":"Service Portability Service portability is the process of migrating a running service and its data from one cloud service provider to another service provider. xpanse implements the service portability using a workflow engine, which supports two different use cases Port service between two different regions with the same cloud service provider.Port service between two different cloud service providers.","keywords":"","version":"Next"},{"title":"Spring Profiles","type":0,"sectionRef":"#","url":"/xpanse/docs/spring-profiles","content":"","keywords":"","version":"Next"},{"title":"Configuration Properties​","type":1,"pageTitle":"Spring Profiles","url":"/xpanse/docs/spring-profiles#configuration-properties","content":" All profiles and the configuration properties that are part of it can be found at this place. Configuration values can be overridden with methods supported by Spring framework as described here.  ","version":"Next","tagName":"h2"},{"title":"Activating Profiles​","type":1,"pageTitle":"Spring Profiles","url":"/xpanse/docs/spring-profiles#activating-profiles","content":" Spring provides multiple ways to activate profiles. The list of profile names which we intend to activate can be passed as  VM argument.Program argument.Environment variable.  # VM argument java -Dspring.profiles.active=profile1,profile2 -jar xpanse-runtime-*-SNAPSHOT.jar # Program argument java -jar xpanse-runtime-*-SNAPSHOT.jar --spring.profiles.active=profile1,profile2 # Environment variable export SPRING_PROFILE_ACTIVE=profile1,profile2  ","version":"Next","tagName":"h2"},{"title":"Supported Clouds","type":0,"sectionRef":"#","url":"/xpanse/docs/supported-clouds","content":"Supported Clouds xpanse will be available on several cloud providers out of the box. Here's the list of the cloud providers supporting OSC: Huawei CloudOpenstack powered cloudFlexibleEngineRegio CloudPlus Server","keywords":"","version":"Next"},{"title":"Workflows","type":0,"sectionRef":"#","url":"/xpanse/docs/workflows","content":"","keywords":"","version":"Next"},{"title":"Activiti Workflow Engine​","type":1,"pageTitle":"Workflows","url":"/xpanse/docs/workflows#activiti-workflow-engine","content":" xpanse uses Activti as our BPMN tool. All workflows can be defined as standard BPMN files.  ","version":"Next","tagName":"h2"},{"title":"Migration Workflow​","type":1,"pageTitle":"Workflows","url":"/xpanse/docs/workflows#migration-workflow","content":" Activiti workflow engine is used for service migration process where an existing service is migrated to a new CSP. This involves multiple steps such as new service deployment, old service destroy, old data import, etc.  BPMN files can be found here.  ","version":"Next","tagName":"h2"},{"title":"APIs to Control Workflow​","type":1,"pageTitle":"Workflows","url":"/xpanse/docs/workflows#apis-to-control-workflow","content":" xpanse provides APIs which can be used by end users to check workflows that have failed and needs end user to handle the errors. Based on the situation can the users then either close the workflow or retry the workflow.  All APIs related to workflow can be found here. ","version":"Next","tagName":"h2"},{"title":"Webhooks","type":0,"sectionRef":"#","url":"/xpanse/docs/webhooks","content":"","keywords":"","version":"Next"},{"title":"Security​","type":1,"pageTitle":"Webhooks","url":"/xpanse/docs/webhooks#security","content":" All webhooks are protected with HMAC by default. The clients of the webhook must know the secret key from xpanse and use this secret key to generate signature.  ","version":"Next","tagName":"h2"},{"title":"Structure of HMAC signature​","type":1,"pageTitle":"Webhooks","url":"/xpanse/docs/webhooks#structure-of-hmac-signature","content":" x-signature: algorithm=HmacSHA256;headers=x-nonce-signature x-timestamp-signature;signature=d4fb456bc7621ed1c8099e8a1136997d6ad9b8613fc79a49b39833d4cb36080a   The name of the header - x-signatureIt must contain algorithm name, headers used in the signature and then the actual signature. Each key and its value must be equals = separated, and each key-value pair must be separated with semicolon ;  Algorithm name must be a valid HMAC algorithm name.headers here refer to other headers sent with the request that are part of the signature payload.  ","version":"Next","tagName":"h3"},{"title":"Format of signature data​","type":1,"pageTitle":"Webhooks","url":"/xpanse/docs/webhooks#format-of-signature-data","content":" Clients must first prepare the data in plain text and then sign it with the secret key to generate the signature. The format of the data is  value of headers in the signature. Each header must be in a newline.Full URL to which the webhook request will be sent in a new line.Payload a string in a new line (request body).  multi-line string The text to be signed must be a multi-line string with each line containing only one piece of information.  ","version":"Next","tagName":"h3"},{"title":"How validation works​","type":1,"pageTitle":"Webhooks","url":"/xpanse/docs/webhooks#how-validation-works","content":" The client generates the signature based on the data such as headers, webhook URL and payload (request body). The format of the signature's plain text data and the secret key is agreed between xpanse and other applications which will send data to xpanse via webhook APIs.  On receiving the request, xpanse again tries to generate signature data from received data and generate the signature again. If this signature value matches to the signature value sent in the request header, then the request is accepted. Otherwise, rejected with HTTP 401.  ","version":"Next","tagName":"h3"},{"title":"Configuration for HMAC Key​","type":1,"pageTitle":"Webhooks","url":"/xpanse/docs/webhooks#configuration-for-hmac-key","content":" xpanse must configure the secret key for HMAC signature signing using the property xpanse.webhook.hmac.request.signing.key.  A random value can be generated using the below command on command line. This will give a 32 byte (256 bit) secret key.  openssl rand -hex 32   Once the key is generated and finalized, it must be used on the client applications as described in their documentation.  Sample key for non-production environments To provide better developer experience, the dev spring profile provides a sample key which can be used in non-production environments. ","version":"Next","tagName":"h2"},{"title":"UI","type":0,"sectionRef":"#","url":"/xpanse/docs/ui","content":"","keywords":"","version":"Next"},{"title":"Application Stack​","type":1,"pageTitle":"UI","url":"/xpanse/docs/ui#application-stack","content":" Project is built using ReactJS library. As we use TypeScript here, we must ensure all objects have its type explicit defined.  GUI components are built using antd library.  ","version":"Next","tagName":"h2"},{"title":"Authentication and Authorization​","type":1,"pageTitle":"UI","url":"/xpanse/docs/ui#authentication-and-authorization","content":" Auth can be controlled by VITE_APP_AUTH_DISABLED configuration property. This property is also already configured with appropriate values in different configuration files.  ","version":"Next","tagName":"h2"},{"title":"Disable​","type":1,"pageTitle":"UI","url":"/xpanse/docs/ui#disable","content":" When we start the UI with the default run command npm run start, then the UI will start with no authentication and authorization. This must be used only for testing and development purposes.  Default Roles When authentication and authorization is disabled, the test user will then have all rolesby default.  ","version":"Next","tagName":"h3"},{"title":"OAuth​","type":1,"pageTitle":"UI","url":"/xpanse/docs/ui#oauth","content":" Authentication and authorization are protected by an OAuth provider. The default implementation uses Zitadel as the OAuth provider.  Zitadel Configuration​  Before we can start using the UI, we must ensure Zitadel instance that we consider using, has all the required configuration. Details can be found here.  For local environments, the access_tokens are stored with in the browser storage. This isn't safe, but it's enough for non-production environments and for debugging purposes.  For production environments, we must use Service Workers which will block anyone from accessing the token. This can be enabled by making the following configurations.  VITE_APP_AUTH_USE_SERVICE_WORKER_ONLY=true   and the file OidcTrustedDomains.js must be updated with correct information.oidcDomains must have the identity provider URL and accessTokenDomains must have the xpanse API server URL.  OidcTrustedDomains.js changes not necessary If the same must be used with our official docker images, then this file need not be touched. It will be automatically set from the environment variables.  ","version":"Next","tagName":"h3"},{"title":"Configuration Properties​","type":1,"pageTitle":"UI","url":"/xpanse/docs/ui#configuration-properties","content":" All required configuration parameters must be added to .env file here. Even if we don't have a valid default value, we must just add empty value. This file serves as a reference to all required properties.  No secrets in configuration file Since React is compiled to a static app, all configuration values can be seen directly in the browser too. Therefore, no secure configurations such as passwords, keys, etc. must be added here.  ","version":"Next","tagName":"h2"},{"title":"Setting Configuration Values​","type":1,"pageTitle":"UI","url":"/xpanse/docs/ui#setting-configuration-values","content":" .env Files​  Set values in the .env files. All default values are set in .env files. These are automatically loaded by React and there is no need to do anything for this to be loaded.For non-default properties or to override the values is .env, we can set the values in new .env files and load them using env-cmd framework which will automatically inject the variables. Example can be found here  Default configuration location .env files must be used only for default configurations or for development environment configurations values.  Environment Variables​  All variables can be overridden by setting environment variables and then running the npm run start for development or with docker run for production.  ","version":"Next","tagName":"h3"},{"title":"Getting Configuration Values​","type":1,"pageTitle":"UI","url":"/xpanse/docs/ui#getting-configuration-values","content":" We've a custom implementation which reads the configuration from all sources and provides a unified configuration map. Only this must be used for reading configuration from the React app.  Implementation can be found here.  ","version":"Next","tagName":"h3"},{"title":"Starting local development server​","type":1,"pageTitle":"UI","url":"/xpanse/docs/ui#starting-local-development-server","content":" In the project directory, you can run the below command to start the local development server. This also additionally needs nodejs to be installed on the development machine.  ","version":"Next","tagName":"h2"},{"title":"No Auth​","type":1,"pageTitle":"UI","url":"/xpanse/docs/ui#no-auth","content":" To start local development server without any auth, use the below command.  $ npm run start   ","version":"Next","tagName":"h3"},{"title":"Local OAuth​","type":1,"pageTitle":"UI","url":"/xpanse/docs/ui#local-oauth","content":" To start local development server with local OAuth server, then update the VITE_APP_ZITADEL_CLIENT_ID value in the .env.zitadel-local fie and then start the server with below command.  $ npm run start-with-zitadel-local   ","version":"Next","tagName":"h3"},{"title":"Testbed OAuth​","type":1,"pageTitle":"UI","url":"/xpanse/docs/ui#testbed-oauth","content":" If you wish to use our central Zitadel testbed instance, then simply start the application with the below command. For this you would need an account on our testbed instance.  $ npm run start-with-zitadel-testbed   Open http://localhost:3000 to view it in the browser.  ","version":"Next","tagName":"h3"},{"title":"Static Code Analysis​","type":1,"pageTitle":"UI","url":"/xpanse/docs/ui#static-code-analysis","content":" We use eslint and knip to statically analyze code. Always run the below command locally to ensure the changes made results in no errors. This will also validate the code format. In case of any errors, the CI pipeline will fail when a PR is opened.   npx eslint . --ext .js,.jsx,.ts,.tsx --config package.json --max-warnings=0 npx knip   ","version":"Next","tagName":"h3"},{"title":"Code Formatting​","type":1,"pageTitle":"UI","url":"/xpanse/docs/ui#code-formatting","content":" We use prettier to format our UI code. To auto format the code, you can run the below command.  npx prettier --config .prettierrc --write .   ","version":"Next","tagName":"h3"},{"title":"Generate Rest Client for xpanse API​","type":1,"pageTitle":"UI","url":"/xpanse/docs/ui#generate-rest-client-for-xpanse-api","content":" We use the OpenAPI generator to generate data models and rest client from the OpenAPI JSON file. The following steps must be followed to generate a new client and data models whenever there is a new version if the swagger JSON.  Start xpanse application with the profiles - oauth,zitadel,dev,zitadel-localOpen the OpenAPI doc in browser - http://localhost:8080/v3/api-docsGet the JSON format of the OpenAPI from raw data tab.Copy the OpenAPI file to the OpenAPI JSON File location.Run the code generator as below ./scripts/generate_api_client.sh This script will generate all required models and client, add license headers and format newly generated files.  ","version":"Next","tagName":"h3"},{"title":"Build for production​","type":1,"pageTitle":"UI","url":"/xpanse/docs/ui#build-for-production","content":" The only recommended way to run UI in production is to use the docker image  ","version":"Next","tagName":"h2"},{"title":"Docker Image​","type":1,"pageTitle":"UI","url":"/xpanse/docs/ui#docker-image","content":" Docker image for the UI project is based on NGINX base image. This is because the project only serves static content.  As part of our UI release process, we deliver our official images to GitHub packages. All available images can be found here.  ","version":"Next","tagName":"h2"},{"title":"Run UI Container​","type":1,"pageTitle":"UI","url":"/xpanse/docs/ui#run-ui-container","content":" Container runs the application on port 3000 by default and exposes the UI using HTTP. We must use another SSL load balancer to expose the UI for end users.  Production configuration values must be passed as environment variables to docker run for the below vars using -e option.  VITE_APP_ZITADEL_AUTHORITY_NAME # URL for the Oauth provider VITE_APP_ZITADEL_CLIENT_ID # Client ID provided by the Oauth provider for UI VITE_APP_XPANSE_API_URL # xpanse API URL VITE_APP_AUTH_USE_SERVICE_WORKER_ONLY=true # for production. Otherwise, this step config can be ignored.   docker pull ghcr.io/eclipse-xpanse/xpanse-ui:${version} docker run -d --name ui xpanse-ui   ","version":"Next","tagName":"h3"},{"title":"Application Logs​","type":1,"pageTitle":"UI","url":"/xpanse/docs/ui#application-logs","content":" All logs from NGINX are routed to stdout by default. Using the below command, all application logs can be viewed.  docker logs ui   ","version":"Next","tagName":"h3"},{"title":"Non production environments without HTTPS​","type":1,"pageTitle":"UI","url":"/xpanse/docs/ui#non-production-environments-without-https","content":" The OAuth client libraries used by the UI are highly secure and don't allow using plain HTTP except for localhost. For all other test purposes when there is a necessity to use HTTP together with an IP or host name that's not localhost, then the browser configuration must be changed as below to consider that IP or host name as a secure origin.  Go to chrome://flags/#unsafely-treat-insecure-origin-as-secure.Add the origin which you want to treat as secure that is, http://example.com:8080.Restart chrome.  ","version":"Next","tagName":"h2"},{"title":"Unit and Integration Tests​","type":1,"pageTitle":"UI","url":"/xpanse/docs/ui#unit-and-integration-tests","content":" We use playwright to write unit and integration tests for the UI components. Please consider the following points in writing tests  Use Page Object Models (POM) for each component.Consider stable locators for elements.Use the default codegen and mock recorder from playwright to ensure faster development.  Running tests​  npx playwright test   Run Tests with inbuilt UI - Necessary for debugging​  npx playwright test --ui   Generating locators and actions for tests​  Use playwright's inbuilt codegen to generate locators for DOM elements. Start UI app locally first and then run  npx playwright codegen http://localhost:3000   Mocking API responses​  It's easier to capture API responses to mock them after that in the unit tests rather than manually creating mock.  This command will start the browser, and then we must perform actions on the UI that are necessary to be mocked. It will then record the API responses into test.har which can be then used in the tests.  npx playwright open --save-har=test.har --save-har-glob=&quot;**/xpanse/**&quot; http://localhost:3000  ","version":"Next","tagName":"h2"}],"options":{"id":"default"}}